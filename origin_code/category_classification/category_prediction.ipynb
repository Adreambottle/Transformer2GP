{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampling\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from simpletransformers.ner import NERModel,NERArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class_features = sampling.read_data(\"config/config-class-features.json\")\n",
    "config_class_name = sampling.read_data(\"config/config-class-name.json\")\n",
    "config_classinfo = sampling.read_data(\"config/config-classinfo.json\")\n",
    "config_numeric_fields = sampling.read_data(\"config/config-numeric-fields.json\")\n",
    "config_dynamic_units = sampling.read_data(\"config/config-dynamic-units.json\")\n",
    "pair_params = sampling.read_data(\"config/pair-params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_all_categories 37\n",
      "query数量： 37\n",
      "label数量： 37\n",
      "problematic set()\n",
      "[['2/2kHz', 'BANDPASS', 'NO', '50 OHM', 'Active Filters', 'e0', '-13.0mV'], ['8/4µF', '4141', 'POLARIZED', '9/7', 'e3', 'IEC60384-4'], ['+-5', '75.5MHz', 'NO', 'MIL-STD-883', 'e4'], ['Converters', '0.015%', '12.0', '3.33µs', 'YES', 'NOT SPECIFIED', '-2.5', '9.57mm', 'DIP'], ['5.0V', 'BIPOLAR', '12.0', 'CHIP CARRIER', 'LDCC28,.5SQ'], ['+-6', '5.0uV', 'VOLTAGE-MODE', 'CAN ALSO OPERATE FROM A 15V NOMINAL SUPPLY', '260', 'L BEND'], ['1.0', 'BIDIRECTIONAL', 'OPEN-COLLECTOR', 'PLASTIC/EPOXY', '3.375kV', 'Tin/Lead (Sn/Pb)'], ['2.161e-05µF', '9/7%', '33.0kV', '10/1', 'SMT', 'BUSSED C NETWORK'], ['232.5ohm', '1/9%', 'THIN FILM', '75.0mV', 'GULL WING'], ['连接辅助', 'PROTECTIVE COVER', 'FEMALE', '362.5', 'COAX', 'NO', 'NO', '75.0', '175.0°C', '-65.0°C'], ['BIPOLAR', '3.3V', 'STS-1/OC-1', '51840.0', 'J BEND', '1', 'SONET;SDH', '85.0°C', '-40.0°C'], ['22.25', '0.905%', 'HTSSOP', '85.0', 'Consumer Circuits', 'INDUSTRIAL', 'Not Qualified', 'GULL WING'], ['2.489', 'POLYBUTYLENE TEREPHTHALATE', 'FEMALE', '10/1', '0.77', 'GENERAL PURPOSE', '3X1 HORIZONTAL'], ['DIE', '8.0ns', 'MATTE TIN', 'UNSPECIFIED'], ['1000 OHMS CT', '250.0kHz', '4/10kHz', '150', '24.9:1', '-55.0', '107.5', 'TUBE'], ['13.15nW', '2.0', '10/2', '1.0', 'QUAD', '-40.0°C', 'NO'], ['信号电路', '1/4', '9/17V', 'YES', 'SOP14,.25', '3.0'], ['1.455', '2/7', '87.5kV', '1.0', 'e3'], ['3.3kV', '2.5kV', '6/8Ah', '150.0mA', '72.5°C', '-20.0°C', 'PRIMARY', '61.75mm', 'CAPACITY MAX AT 2 V'], ['5.0uV', '16.0', '10.0', 'NO', 'FLATPACK', 'DIP', 'J BEND'], ['2', '0.65', '4/4mm', '-47.5', '95.0', '94V-0', '3/8'], ['1.298mV', '2/5', '1.673kV', '2.0', '0.0', '97.5', '2', 'SINGLE', 'NO'], ['diodes', '1.1', '44.0', 'SILICON', '13.25kA', '1/10°C', '8/8°C', 'Bridge Rectifier Diodes', 'BRIDGE RECTIFIER DIODE', '260'], ['0.03µA', '5/2V/us', '1.0', '-10.0kV', 'RECTANGULAR', 'NO', 'YES'], ['Microcontrollers', '8.0', '3.4kV', '8.0', '260', 'NO'], ['NO', '5.0kV', 'BIDIRECTIONAL', 'SOP24,.4', 'FF/Latches', '3.075V'], ['7/6', '5.0', '14.0', '4.15', 'COMMERCIAL', '10', '1/9'], ['5000.0MHz', '1.5', 'N MALE', '200.0', 'FV50', '3', '32.35', '6/5'], ['8/9%', '10.0kHz', '-20.0V', '5250.0MHz', '7/1MHz', '1.5dB'], ['3.15mV', '1.5', '10/5', '3.0', 'LCC48,.27SQ,20', 'INDUSTRIAL'], ['4/2µF', 'C0G', '37.5kV', 'Other Cap. & Volt. Available On Request', '30ppm/Celppm/°C'], ['50 OHM', '50 OHM', '5.0', '20.0', '2.643mm'], ['3 PPM/YEAR', '40.005', '5000.0ppm', '0.4', 'THROUGH HOLE MOUNT', 'CERAMIC RESONATOR', '14.195MHz', '2.0MHz'], ['240.0uV', '6.5uA', 'INSTANTANEOUS', '60.0mV', 'TA35-C073KJ01C0', 'IP40; IP00', 'THERMAL CIRCUIT BREAKER'], ['Connect', 'INCONEL 600', 'PANEL', '55', 'YES', 'RECEPTACLE-PLUG', 'NO'], ['37', '27', 'CABLE', 'MALE', 'CIRCULAR CONNECTOR', 'NO', '1.285'], ['1.0', '58034', '4.15mV', 'NOT SPECIFIED', 'ENABLE LOW']]\n",
      "['Active Filters', 'Aluminum Electrolytic Capacitors', 'Analog Computational Functions', 'Analog to Digital Converters', 'Analog Transmission Interfaces', 'Analog Waveform Generation Functions', 'Arithmetic Circuits', 'Array/Network Capacitors', 'Array/Network Resistors', 'Assembly Items', 'ATM/SONET/SDH ICs', 'Audio Control ICs', 'Audio/RCA Connectors', 'Audio Synthesizer ICs', 'Audio Transformers', 'Audio/Video Amplifiers', 'Automotive Circuits', 'Auto Transformers', 'Batteries', 'Bit-Slice Processors', 'Board Stacking Connectors', 'Breakover Diodes', 'Bridge Rectifier Diodes', 'Buffer Amplifiers', 'Bus Controllers', 'Bus Driver/Transceivers', 'Bus Terminators', 'Cavity Filters', 'Cavity Oscillators', 'Cellular Telephone Circuits', 'Ceramic Capacitors', 'Ceramic Filters', 'Ceramic Resonators', 'Circuit Breaker', 'Circular Adapters', 'Circular Connectors', 'Clock Drivers']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_rs_list_of_all_category():\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "    # 读取所有catogory文件\n",
    "    path = os.getcwd()\n",
    "    files= os.listdir('./formatData') #得到文件夹下的所有文件名称\n",
    "    rs_list = []\n",
    "    \n",
    "    for file in files: #遍历文件夹\n",
    "        file_path = os.path.join(path, 'formatData/'+file)\n",
    "        if os.path.isfile(file_path): #判断是否是文件夹，不是文件夹才打开\n",
    "                rs = sampling.sampling(file,0.6)\n",
    "                rs_list.append((rs,file))\n",
    "                \n",
    "    print('num_of_all_categories',len(rs_list))\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('num_of_all_categories'+str(len(rs_list))+'\\n')\n",
    "        \n",
    "        \n",
    "    return rs_list\n",
    "\n",
    "def get_input_from_sampling(input_size: int):\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "    rs_list = get_rs_list_of_all_category()\n",
    "    \n",
    "    # input_size为1表示从每一个category抽取一个样本\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    problematic = set()\n",
    "    \n",
    "    for i in range(input_size):\n",
    "        for rs in rs_list:\n",
    "            try:\n",
    "                item=rs[0].random_sampling()\n",
    "            except Exception as e:\n",
    "                problematic.add(rs[1])\n",
    "                continue\n",
    "            # print(item)\n",
    "            tmp_list = []\n",
    "            for key, val in item.items():\n",
    "                if key == 'class':\n",
    "                    continue\n",
    "                elif key == 'category':\n",
    "                    label_list.append(val)\n",
    "                else:\n",
    "                    tmp_list.append(str(val))\n",
    "            text_list.append(tmp_list)\n",
    "    print('query数量：',len(text_list))\n",
    "    print('label数量：',len(label_list))\n",
    "    print('problematic',problematic)\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('query数量'+str(len(text_list))+'\\n')\n",
    "        f.write('label数量'+str(len(label_list))+'\\n')\n",
    "        f.write('problematic'+str(problematic)+'\\n')\n",
    "    return text_list,label_list\n",
    "\n",
    "a,b=get_input_from_sampling(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_process(config_classinfo,model_checkpoint,tokenizer,train_size,test_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    files= os.listdir('./inputData')\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print('torch.cuda.is_available()',gpu_available)\n",
    "    \n",
    "    with open('running_output.txt','w') as f:\n",
    "        f.write(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+'\\n')\n",
    "        f.write('torch.cuda.is_available():'+str(gpu_available)+'\\n')\n",
    "    \n",
    "    #准备训练数据\n",
    "    print('train dataset\\n')\n",
    "    if 'train_encodings.pt' in files and 'train_labels.csv' in files:\n",
    "        print('read train dataset\\n')\n",
    "        train_encodings = torch.load('./inputData/train_encodings.pt')\n",
    "        train_labels = pd.read_csv('./inputData/train_labels.csv',index_col=0)\n",
    "        train_labels = train_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate train dataset\\n')\n",
    "        train_dataset,train_labels = get_input_from_sampling(train_size)\n",
    "        train_encodings = tokenizer(train_dataset, is_split_into_words=True,add_special_tokens=False,\n",
    "                        padding=True,truncation=False,return_tensors=\"pt\")\n",
    "        torch.save(train_encodings, './inputData/train_encodings.pt')\n",
    "        pd.DataFrame(train_labels).to_csv('./inputData/train_labels.csv')\n",
    "        print('train dataset saved')\n",
    "        \n",
    "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "    \n",
    "    print('test dataset')\n",
    "    if 'test_encodings.pt' in files and 'test_labels.csv' in files:\n",
    "        print('read test dataset\\n')\n",
    "        test_encodings = torch.load('./inputData/test_encodings.pt')\n",
    "        test_labels = pd.read_csv('./inputData/test_labels.csv',index_col=0)\n",
    "        test_labels = test_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate test dataset\\n')\n",
    "        test_dataset,test_labels = get_input_from_sampling(test_size)\n",
    "        test_encodings = tokenizer(test_dataset, is_split_into_words=True,add_special_tokens=False,\n",
    "                        padding=True,truncation=False,return_tensors=\"pt\")\n",
    "        torch.save(test_encodings, './inputData/test_encodings.pt')\n",
    "        pd.DataFrame(test_labels).to_csv('./inputData/test_labels.csv')\n",
    "        print('test dataset saved\\n')\n",
    "\n",
    "    # encode the labels\n",
    "    categories = []\n",
    "#     for val in config_classinfo.values():\n",
    "#         categories.extend(val)\n",
    "    le = LabelEncoder()\n",
    "    categories = set(train_labels)|set(test_labels)\n",
    "    le.fit(list(categories))\n",
    "    label_cnt = le.classes_.shape[0]\n",
    "\n",
    "    train_labels_encoded = list(le.transform(train_labels))\n",
    "    print(len(train_labels_encoded))\n",
    "\n",
    "    test_labels_encoded = list(le.transform(test_labels))\n",
    "    print(len(test_labels_encoded))\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('category数量'+str(label_cnt)+'\\n')\n",
    "    \n",
    "    return le,label_cnt,train_encodings,test_encodings,train_labels_encoded,test_labels_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class processDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClass(dict,cat):\n",
    "    for key, val in dict.items():\n",
    "        if cat in val:\n",
    "            return key\n",
    "    return 'unknown_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    global le,config_classinfo\n",
    "    cat_labels = list(le.inverse_transform(labels))\n",
    "    # print(preds)\n",
    "    cat_preds = list(le.inverse_transform(preds))\n",
    "    cat_report = classification_report(cat_labels, cat_preds,output_dict=True)\n",
    "    cat_report2 = classification_report(cat_labels, cat_preds)\n",
    "    cat_result = {'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall}            \n",
    "    #需要继续改\n",
    "    class_preds = [getClass(config_classinfo,item) for item in cat_preds]\n",
    "    class_labels = [getClass(config_classinfo,item) for item in cat_labels]\n",
    "    class_report = classification_report(class_labels, class_preds,output_dict=True)\n",
    "    class_report2 = classification_report(class_labels, class_preds)\n",
    "    class_result = {'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall}\n",
    "    \n",
    "    \n",
    "    with open('cat_classification_report.txt','w') as f: \n",
    "        js = json.dumps(cat_report) \n",
    "        f.write(js)\n",
    "    with open('cat_classification_report_string_format.txt','w') as f: \n",
    "        f.write(cat_report2)\n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('cat_result:'+str(cat_result)+'\\n')\n",
    "    \n",
    "    with open('class_classification_report.txt','w') as f: \n",
    "        js = json.dumps(class_report) \n",
    "        f.write(js)\n",
    "    with open('class_classification_report_string_format.txt','w') as f: \n",
    "        f.write(class_report2)\n",
    "    with open('class_running_output.txt','w') as f:\n",
    "        f.write('class_result:'+str(class_result)+'\\n')\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "test dataset\n",
      "37\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\training_args.py:347: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n",
      "D:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 43.221421003341675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_preds ['Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators']\n",
      "cat_labels ['Active Filters', 'Aluminum Electrolytic Capacitors', 'Analog Computational Functions', 'Analog to Digital Converters', 'Analog Transmission Interfaces', 'Analog Waveform Generation Functions', 'Arithmetic Circuits', 'Array/Network Capacitors', 'Array/Network Resistors', 'Assembly Items']\n",
      "class_preds ['Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators']\n",
      "class_labels ['Filters', 'Capacitors', 'Signal Circuits', 'Converters', 'Telecommunication Circuits', 'Signal Circuits', 'Logic', 'Capacitors', 'Resistors', 'Connector Support']\n",
      "{'eval_loss': 3.6519815921783447, 'eval_accuracy': 0.02702702702702703, 'eval_f1': 0.02702702702702703, 'eval_precision': 0.02702702702702703, 'eval_recall': 0.02702702702702703, 'epoch': 3.0}\n",
      "cat_preds ['Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators', 'Cavity Oscillators']\n",
      "cat_labels ['Active Filters', 'Aluminum Electrolytic Capacitors', 'Analog Computational Functions', 'Analog to Digital Converters', 'Analog Transmission Interfaces', 'Analog Waveform Generation Functions', 'Arithmetic Circuits', 'Array/Network Capacitors', 'Array/Network Resistors', 'Assembly Items']\n",
      "class_preds ['Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators', 'Oscillators']\n",
      "class_labels ['Filters', 'Capacitors', 'Signal Circuits', 'Converters', 'Telecommunication Circuits', 'Signal Circuits', 'Logic', 'Capacitors', 'Resistors', 'Connector Support']\n",
      "{'eval_loss': 3.6686158180236816, 'eval_accuracy': 0.02702702702702703, 'eval_f1': 0.02702702702702703, 'eval_precision': 0.02702702702702703, 'eval_recall': 0.02702702702702703, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    set_seed(1024)\n",
    "    model_checkpoint = \"albert-base-v2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    le,label_cnt,train_encodings,test_encodings,train_labels_encoded,test_labels_encoded = input_process(config_classinfo,\n",
    "                                                                                                         model_checkpoint,tokenizer,10000,2000)\n",
    "\n",
    "    print('input_process_finished\\n')\n",
    "    train_dataset = processDataset(train_encodings, train_labels_encoded)\n",
    "    test_dataset = processDataset(test_encodings, test_labels_encoded)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=label_cnt)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=5e-3,\n",
    "        weight_decay=0.01,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=150000,\n",
    "        save_total_limit=5,\n",
    "        no_cuda=False,\n",
    "        seed=1024,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print('Training begins\\n')\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    end = time.time()\n",
    "    print(f\"training time: {end - start}\")\n",
    "\n",
    "    train_result = trainer.evaluate(train_dataset)\n",
    "    print(train_result)\n",
    "    test_result = trainer.evaluate(test_dataset)\n",
    "    print(test_result)\n",
    "\n",
    "    trainer.save_model()\n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write(f\"training time: {end - start}\"+'\\n')\n",
    "        f.write('train_dataset'+str(train_result)+'\\n')        \n",
    "        f.write('test_dataset'+str(test_result)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_prediction_result(model):\n",
    "#     \"\"\"\n",
    "#     model: the trained model\n",
    "#     returns the predicted result\n",
    "#     \"\"\"\n",
    "#     rs_list = get_rs_list_of_all_category()\n",
    "#     rs = random.choice(rs_list)\n",
    "#     item = rs.random_sampling()\n",
    "#     truth_result = pd.DataFrame.from_dict(item,orient='index').reset_index()\n",
    "#     truth_result.columns = ['output_truth','input']\n",
    "#     truth_result['input'] = truth_result['input']\n",
    "#     print(truth_result)\n",
    "#     input_sentence = list(item.values())\n",
    "#     input_sentence = [str(x) for x in input_sentence]\n",
    "#     predictions, raw_outputs = model.predict([input_sentence],split_on_space= False )\n",
    "    \n",
    "#     pred_dict={}\n",
    "#     for item in predictions[0]:\n",
    "#         pred_dict.update(item)\n",
    "#     pred_result = pd.DataFrame.from_dict(pred_dict,orient='index').reset_index()\n",
    "#     pred_result.columns = ['input','output_pred']\n",
    "#     print(pred_result)\n",
    "#     show_result = pd.merge(pred_result,truth_result,how='right',on='input')\n",
    "#     return show_result\n",
    "# show_prediction_result(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('classification_report.txt', 'r') as f:\n",
    "#     js = f.read()\n",
    "#     dic = json.loads(js)   \n",
    "#     print(dic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_transformer",
   "language": "python",
   "name": "sim_transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
