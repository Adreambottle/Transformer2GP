{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 载入全局数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import random\n",
    "from transformers import BertModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, \\\n",
    "    trainer_callback, DataCollatorForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import json\n",
    "import gc\n",
    "import sys\n",
    "import re\n",
    "from seqeval.metrics import accuracy_score, classification_report\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# In[2]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_from_file_single(path, data_num, text_list, label_list, class_list, file_num, train_flag):\n",
    "    \"\"\"\n",
    "    暂时不用，用于读取单个的文件\n",
    "    这个函数是用来从 Cap 和 Res 里面读取原始数据，生成一个分割好的 data\n",
    "    每次调用的时候都需要重新读取一遍数据\n",
    "    path:        现在是: \"preprocess/standard_cap.json\"\n",
    "                        \"preprocess/standard_res.json\"\n",
    "    data_num:    是数据的个数，每个文件里面有多少条数据\n",
    "    file_num:    是生成几个拆分好的文件包\n",
    "    train_flag:  是用于train还是test\n",
    "    class_list:  是什么\n",
    "    text_list:   把生成的sample的'description'添加到text_list\n",
    "    label_list:  把生成的sample的 class label 添加到label_list\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "\n",
    "        # standard_data 是内存需要读取的文件\n",
    "        standard_data = json.loads(f.read())\n",
    "        # print('sys.getsizeof(standard_data)',sys.getsizeof(standard_data))\n",
    "        # sample_data = random.sample(standard_data,343*data_num)\n",
    "\n",
    "    print('\\tsys.getsizeof(standard_data)', sys.getsizeof(standard_data))\n",
    "    # 这个是standard_data的数据大小\n",
    "\n",
    "    standard_data_length = len(standard_data)\n",
    "    print('\\tlen of standard_data:', standard_data_length)\n",
    "    # 这个是standard_data的数据长度\n",
    "\n",
    "    # 不同category的匹配\n",
    "    execute_num = 1\n",
    "    # execute_num = 338\n",
    "\n",
    "    if train_flag:\n",
    "        # 如果是train的话，将数据进行拆分，生成一个sample_data\n",
    "        lower_bound = file_num * data_num * execute_num\n",
    "        upper_bound = min(standard_data_length, (file_num + 1) * data_num * execute_num)\n",
    "        sample_data = standard_data[lower_bound:upper_bound]\n",
    "\n",
    "    else:\n",
    "        # 如果是test的话，只选取最前面的部分\n",
    "        upper_bound = min(standard_data_length, data_num * execute_num)\n",
    "        sample_data = standard_data[0:upper_bound]\n",
    "\n",
    "    # 在内存读取 sample_data 之后，直接将原始数据删除\n",
    "    del standard_data\n",
    "    gc.collect()\n",
    "\n",
    "    for item in sample_data:\n",
    "        # 将 sample_data 中的 class，label 和 text 部分分别添加到列表里面\n",
    "        # item = sample_data[0]\n",
    "\n",
    "        # 生成 value 和 key 的空列表\n",
    "        tmp_val = []\n",
    "        tmp_key = []\n",
    "        shuffled_item = list(item.items())\n",
    "        random.shuffle(shuffled_item)\n",
    "\n",
    "        for key, val in shuffled_item:\n",
    "            if key == 'class':\n",
    "                class_list.append(val)\n",
    "            else:\n",
    "                tmp_val.append(str(val))\n",
    "                tmp_key.append(key)\n",
    "        text_list.append(tmp_val)\n",
    "        label_list.append(tmp_key)\n",
    "\n",
    "    return text_list, label_list, class_list\n",
    "\n",
    "\n",
    "def get_input_from_file(path, data_num, text_list, label_list, class_list, file_num, train_flag):\n",
    "    \"\"\"\n",
    "    用于读取一个路径下的全部文件\n",
    "    这个函数是用来从 Cap 和 Res 里面读取原始数据，生成一个分割好的 data\n",
    "    每次调用的时候都需要重新读取一遍数据\n",
    "    path:        现在是: \"./MultiData/cap\", \"./MultiData/res\"\n",
    "    data_num:    是数据的个数，每个文件里面有多少条数据\n",
    "    file_num:    是生成几个拆分好的文件包\n",
    "    train_flag:  是用于train还是test\n",
    "    class_list:  把生成的sample的 class label 添加到class_list\n",
    "    text_list:   把生成的sample的'description'添加到text_list\n",
    "    label_list:  把生成的sample的 label label 添加到label_list\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    files_json = []\n",
    "    for file in files:\n",
    "        if file[-5:] == \".json\":\n",
    "            files_json.append(file)\n",
    "\n",
    "    files_json_num = 0\n",
    "    for file in files_json:\n",
    "\n",
    "        # file = files_json[0]\n",
    "        # print(f\"\\t读取第{files_json_num}个原始数据\")\n",
    "        files_json_num += 1\n",
    "\n",
    "        with open(path + '/' + file, 'r', encoding='utf-8') as f:\n",
    "\n",
    "            # standard_data 是内存需要读取的文件\n",
    "            standard_data = json.loads(f.read())\n",
    "            # print('sys.getsizeof(standard_data)',sys.getsizeof(standard_data))\n",
    "            # sample_data = random.sample(standard_data,343*data_num)\n",
    "\n",
    "        # print('\\tsys.getsizeof(standard_data)', sys.getsizeof(standard_data))\n",
    "        # 这个是standard_data的数据大小\n",
    "\n",
    "        standard_data_length = len(standard_data)\n",
    "        # print('\\tlen of standard_data:', standard_data_length)\n",
    "        # 这个是standard_data的数据长度\n",
    "\n",
    "        execute_num = 1\n",
    "        # 这个是比例\n",
    "\n",
    "        if train_flag:\n",
    "            # 如果是train的话，将数据进行拆分，生成一个sample_data\n",
    "            lower_bound = file_num * data_num * execute_num\n",
    "            upper_bound = min(standard_data_length, (file_num + 1) * data_num * execute_num)\n",
    "            sample_data = standard_data[lower_bound:upper_bound]\n",
    "\n",
    "        else:\n",
    "            # 如果是test的话，只选取最前面的部分\n",
    "            upper_bound = min(standard_data_length, data_num * execute_num)\n",
    "            sample_data = standard_data[0:upper_bound]\n",
    "\n",
    "        # 在内存读取 sample_data 之后，直接将原始数据删除\n",
    "        del standard_data\n",
    "        gc.collect()\n",
    "\n",
    "        for item in sample_data:\n",
    "            # 将 sample_data 中的 class，label 和 text 部分分别添加到列表里面\n",
    "            # item = sample_data[0]\n",
    "\n",
    "            # 生成 value 和 key 的空列表\n",
    "            tmp_val = []\n",
    "            tmp_key = []\n",
    "            shuffled_item = list(item.items())\n",
    "            random.shuffle(shuffled_item)\n",
    "\n",
    "            for key, val in shuffled_item:\n",
    "                if key == 'class':\n",
    "                    class_list.append(val)\n",
    "                else:\n",
    "                    tmp_val.append(str(val))\n",
    "                    tmp_key.append(key)\n",
    "            text_list.append(tmp_val)\n",
    "            label_list.append(tmp_key)\n",
    "\n",
    "    return text_list, label_list, class_list\n",
    "\n",
    "\n",
    "def get_input_from_sampling(input_size, train_flag, file_num):\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    input_size:  是 train_size\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "\n",
    "    # 设置数据读取的路径，本地和服务器的路径是不一样的\n",
    "    cap_path = ADDRESS[\"raw_path\"][\"cap_path\"]\n",
    "    res_path = ADDRESS[\"raw_path\"][\"res_path\"]\n",
    "\n",
    "    # cap_path = './MultiData/cap'\n",
    "    # res_path = './MultiData/res'\n",
    "\n",
    "    # 将抽取到的结果转化为输入格式\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    class_list = []\n",
    "\n",
    "    if train_flag:\n",
    "        text_list, label_list, class_list = get_input_from_file(cap_path, input_size, text_list,\n",
    "                                                                label_list, class_list, file_num,\n",
    "                                                                train_flag)\n",
    "        text_list, label_list, class_list = get_input_from_file(res_path, input_size, text_list,\n",
    "                                                                label_list, class_list, file_num,\n",
    "                                                                train_flag)\n",
    "    else:\n",
    "        text_list, label_list, class_list = get_input_from_file(cap_path, input_size,\n",
    "                                                                text_list, label_list, class_list,\n",
    "                                                                file_num, train_flag)\n",
    "        text_list, label_list, class_list = get_input_from_file(res_path, input_size,\n",
    "                                                                text_list, label_list, class_list, file_num,\n",
    "                                                                train_flag)\n",
    "\n",
    "    # to shuffle input\n",
    "    res = []\n",
    "    for i in range(len(label_list)):\n",
    "        res.append((text_list[i], label_list[i], class_list[i]))\n",
    "    random.shuffle(res)\n",
    "    for i in range(len(label_list)):\n",
    "        text_list[i] = res[i][0]\n",
    "        label_list[i] = res[i][1]\n",
    "        class_list[i] = res[i][2]\n",
    "\n",
    "    print('query数量：', len(text_list))  # print 的是shuffled之后的数据长度\n",
    "    print('label数量：', len(label_list))\n",
    "    print('class数量：', len(class_list))\n",
    "\n",
    "    #     print('sys.getsizeof(text_list)',sys.getsizeof(text_list))\n",
    "    #     print('sys.getsizeof(label_list)',sys.getsizeof(label_list))\n",
    "\n",
    "    return text_list, label_list, class_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Embedding 和 存储函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(text, labels_before_split, tokenizer):\n",
    "    tokenized_inputs = tokenizer(text, is_split_into_words=True, add_special_tokens=False,\n",
    "                                 padding=True, truncation=False, return_tensors=\"pt\")\n",
    "    labels = []\n",
    "    for i, label in enumerate(labels_before_split):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append('-100')\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    #    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs, labels\n",
    "\n",
    "\n",
    "def show_tokenizer_result(text, labels_before_split, tokenizer):\n",
    "    tokenized_inputs, labels = tokenize_and_align_labels(text, labels_before_split, tokenizer)\n",
    "    for i in range(len(labels_before_split)):\n",
    "        print('text:', text[i])\n",
    "        print('tokenized result:', tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"][i]))\n",
    "        # print('tokenized_inputs:',tokenized_inputs[\"input_ids\"][i])\n",
    "\n",
    "        print('labels_before_split:', labels_before_split[i])\n",
    "        print('labels:', labels[i])\n",
    "        print('\\n%%%%%%%%%%%%%%%%%%%%%\\n')\n",
    "\n",
    "\n",
    "# 读文件里面的数据转化为二维列表\n",
    "def read_list(filename):\n",
    "    \"\"\"\n",
    "    读取一个无txt后缀名的文件\n",
    "    将\"\\t\"作为分隔符，将每一行转为一个list\n",
    "    返回的是一个 list_source\n",
    "    \"\"\"\n",
    "    file1 = open(filename + \".txt\", \"r\")\n",
    "    list_row = file1.readlines()\n",
    "    list_source = []\n",
    "    for i in range(len(list_row)):\n",
    "        column_list = list_row[i].strip().split(\"\\t\")  # 每一行split后是一个列表\n",
    "        list_source.append(column_list)  # 在末尾追加到list_source\n",
    "    file1.close()\n",
    "    return list_source\n",
    "\n",
    "\n",
    "# 保存二维列表到文件\n",
    "def save_list(list1, filename):\n",
    "    \"\"\"\n",
    "    把生成的嵌套list转换成一个txt文件\n",
    "    \"\"\"\n",
    "    file2 = open(filename + '.txt', 'w')\n",
    "    for i in range(len(list1)):\n",
    "        for j in range(len(list1[i])):\n",
    "            file2.write(str(list1[i][j]))  # write函数不能写int类型的参数，所以使用str()转化\n",
    "            file2.write('\\t')  # 相当于Tab一下，换一个单元格\n",
    "        file2.write('\\n')  # 写完一行立马换行\n",
    "    file2.close()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def attrEncoder(all_class_list, item_class, attr):\n",
    "    \"\"\"\n",
    "    就是编码\n",
    "    \"\"\"\n",
    "    if item_class in all_class_list and attr in all_class_list[item_class]:\n",
    "        if item_class == 'Capacitors':\n",
    "            return all_class_list[item_class][attr]\n",
    "        if item_class == 'Resistors':\n",
    "            return all_class_list[item_class][attr] - 10\n",
    "    if attr == '-100':\n",
    "        return -100\n",
    "    return 0\n",
    "\n",
    "\n",
    "def attrDecoder(item_class, attr):\n",
    "    if attr == 0 or attr == -100:\n",
    "        return 'others'\n",
    "    else:\n",
    "        if item_class == 'Resistors':\n",
    "            attr += 10\n",
    "        global all_attrs_dict\n",
    "        return all_attrs_dict[attr]\n",
    "# attrDecoder('Resistors',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### train 和 test 的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_input_process(train_size, tokenizer, file_num):\n",
    "    \"\"\"\n",
    "    读取保存好的数据：【pt-encodings-text】【txt-lables-】\n",
    "    train_size:     get_input_from_sampling 的第一个参数\n",
    "    tokenizer:\n",
    "    file_num\n",
    "    \"\"\"\n",
    "    files = os.listdir('./inputData')\n",
    "    # 打开这个路径下的全部文件\n",
    "\n",
    "    #     with open('running_output.txt','w') as f:\n",
    "    #         f.write(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+'\\n')\n",
    "\n",
    "    # 准备训练数据\n",
    "    print(f'train dataset{file_num}\\n')\n",
    "    if f'train_encodings{file_num}.pt' in files and f'train_labels{file_num}.txt' in files and f'train_class{file_num}.csv' in files:\n",
    "        # 读取数据\n",
    "        print(f'read train dataset{file_num}\\n')\n",
    "        train_encodings = torch.load(f'./inputData/train_encodings{file_num}.pt')\n",
    "        train_labels = read_list(f'./inputData/train_labels{file_num}')\n",
    "        train_class = pd.read_csv(f'./inputData/train_class{file_num}.csv', index_col=0)\n",
    "        train_class = train_class['0'].to_list()\n",
    "\n",
    "    else:\n",
    "        # 生成数据\n",
    "        generate_start_time = time.time()\n",
    "        print(f'generate train dataset{file_num}')\n",
    "        train_dataset, train_labels, train_class = get_input_from_sampling(train_size, train_flag=True,\n",
    "                                                                           file_num=file_num)\n",
    "        # train_encodings = tokenizer(train_dataset, is_split_into_words=True,add_special_tokens=False,\n",
    "        #                        padding=True,truncation=False,return_tensors=\"pt\")\n",
    "        train_encodings, train_labels = tokenize_and_align_labels(train_dataset, train_labels, tokenizer)\n",
    "\n",
    "        # 把 train_text 进行 encoding 编码， 保存成 pt 文件；把 train_label 保存成没有后缀的 txt 文件；把 train_class 保存成 csv 文件\n",
    "        pd.DataFrame(train_class).to_csv(f'./inputData/train_class{file_num}.csv')\n",
    "        save_list(train_labels, f'./inputData/train_labels{file_num}')\n",
    "        torch.save(train_encodings, f'./inputData/train_encodings{file_num}.pt')\n",
    "        generate_end_time = time.time()\n",
    "        print(f'Train dataset saved, using time {generate_end_time - generate_start_time}s.')\n",
    "\n",
    "    # encode the labels\n",
    "    global all_class_list\n",
    "    train_labels_encoded = []\n",
    "    for i in range(len(train_labels)):\n",
    "        #        train_labels_encoded[i] = list(map(lambda x,y,z:attrEncoder(x,y,z),all_class_list,train_class[i],train_labels[i]))\n",
    "        train_labels_encoded.append(\n",
    "            [attrEncoder(all_class_list, train_class[i], train_label) for train_label in train_labels[i]])\n",
    "\n",
    "    print('training dataset is ok\\n')\n",
    "    return train_encodings, train_labels_encoded, train_class\n",
    "\n",
    "\n",
    "def test_input_process(test_size, tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('generate test dataset\\n')\n",
    "    generate_start_time = time.time()\n",
    "    test_dataset, test_labels, test_class = get_input_from_sampling(test_size, train_flag=False, file_num=1)\n",
    "    test_encodings, test_labels = tokenize_and_align_labels(test_dataset, test_labels, tokenizer)\n",
    "\n",
    "    # encode the labels\n",
    "    global all_class_list\n",
    "    test_labels_encoded = []\n",
    "    for i in range(len(test_labels)):\n",
    "        test_labels_encoded.append(\n",
    "            [attrEncoder(all_class_list, test_class[i], test_label) for test_label in test_labels[i]])\n",
    "\n",
    "    print('testing dataset is ok\\n')\n",
    "    return test_encodings, test_labels_encoded, test_class, test_dataset[:1000], test_labels\n",
    "\n",
    "\n",
    "def ClassEncoder(Class):\n",
    "    if Class == 'Capacitors':\n",
    "        return 1\n",
    "    elif Class == 'Resistors':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def ClassDecoder(Class):\n",
    "    if Class == 1:\n",
    "        return 'Capacitors'\n",
    "    elif Class == 2:\n",
    "        return 'Resistors'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "class processDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, classes):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.classes = classes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['classes'] = torch.tensor(self.classes[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NER Class Neural Network 的部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class nerClass(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    定义ner的Class，是torch里面的一个层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, TRAIN_CONFIG):\n",
    "        \"\"\"\n",
    "        config 是个全局变量     config = {'num_labels1': 9, 'num_labels2': 8, 'model': \"prajjwal1/bert-tiny\"}\n",
    "                              config['num_labels1] = 9   分成9类\n",
    "                              config['num_labels1] = 8   分成8类\n",
    "                              config['model'] = \"prajjwal1/bert-tiny\" 预训练的模型\n",
    "        \"\"\"\n",
    "        super(nerClass, self).__init__()\n",
    "\n",
    "        if TRAIN_CONFIG[\"model\"] == \"prajjwal1/bert-tiny\":\n",
    "            self.embedding_length = 128\n",
    "        elif TRAIN_CONFIG[\"model\"] == \"albert-base-v2\":\n",
    "            self.embedding_length = 768\n",
    "        self.num_labels1 = TRAIN_CONFIG['num_labels1']\n",
    "        self.num_labels2 = TRAIN_CONFIG['num_labels2']\n",
    "        self.l1 = BertModel.from_pretrained(TRAIN_CONFIG['model'])\n",
    "\n",
    "        #         self.pre_classifier = torch.nn.Linear(128, 128)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier1 = torch.nn.Linear(self.embedding_length, TRAIN_CONFIG['num_labels1'])  # classifier1是一个线性分类器\n",
    "        self.classifier2 = torch.nn.Linear(self.embedding_length, TRAIN_CONFIG['num_labels2'])\n",
    "\n",
    "    #        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, targets=None, classes=None, device=None):\n",
    "        #         output_1 = self.l1(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "        #                             attention_mask=attention_mask, head_mask=head_mask)\n",
    "        # the hidden states of the last Bert Layer, shape: (bsz, seq_len, hsz)\n",
    "\n",
    "        \"\"\"\n",
    "        input_ids:\n",
    "        attention_mask:\n",
    "        targets:\n",
    "        classes:\n",
    "        device:\n",
    "        \"\"\"\n",
    "        output = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]  # 输出的第0个return\n",
    "        # last_hidden_state (torch.FloatTensor of shape (\n",
    "        #                    batch_size, sequence_length, hidden_size))\n",
    "        # print('hidden_state.size()',hidden_state[:,0].size())\n",
    "        # pooler = self.pre_classifier(pooler)\n",
    "        # pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(hidden_state)\n",
    "        #        print('pooler.size()',pooler.size())\n",
    "\n",
    "        mask1 = torch.eq(classes, 1)  # classes 是输入，判断是否等于 1\n",
    "        mask2 = torch.eq(classes, 2)\n",
    "        #        print('pooler[mask1].size()',pooler[mask1].size())\n",
    "        #        print('pooler[mask2].size()',pooler[mask2].size())\n",
    "\n",
    "        # 这里是两个头，分开进行预测\n",
    "        output1 = self.classifier1(pooler[mask1])  # 是pooler经过mask1筛选后的\n",
    "        output2 = self.classifier2(pooler[mask2])  # 是pooler经过mask2筛选后的\n",
    "        #         print('output1.size()',output1.size())\n",
    "        #         print('output2.size()',output2.size())\n",
    "\n",
    "        #        output1 = output1[mask1]\n",
    "        target1 = targets[mask1]  # targets是一个input的参数\n",
    "        #         print('target1.size()',target1.size())\n",
    "\n",
    "        #        output2 = output2[mask2]\n",
    "        target2 = targets[mask2]\n",
    "        #         print('target2.size()',target2.size())\n",
    "\n",
    "        if output1.size()[0] != 0:\n",
    "            if attention_mask[mask1] is not None:\n",
    "                \"\"\"\n",
    "                # loss_function 是 CrossEntropy, 忽略 -100\n",
    "                \"\"\"\n",
    "                active_loss1 = attention_mask[mask1].view(-1) == 1\n",
    "                #            print('active_loss1',active_loss1.size())\n",
    "                active_logits1 = output1.view(-1, TRAIN_CONFIG['num_labels1'])\n",
    "                #                 print('active_logits1',active_logits1.size())\n",
    "                active_labels1 = torch.where(\n",
    "                    active_loss1, target1.view(-1), torch.tensor(LOSS_FUNCTION.ignore_index).type_as(target1)\n",
    "                )\n",
    "                #                 print('active_labels1',active_labels1.size())\n",
    "                loss1 = LOSS_FUNCTION(active_logits1, active_labels1)\n",
    "            else:\n",
    "                loss1 = LOSS_FUNCTION(output1.view(-1, TRAIN_CONFIG['num_labels1']), target1.view(-1))\n",
    "        else:\n",
    "            # 如果batch_size是0的话，loss1的值就是0\n",
    "            loss1 = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "\n",
    "        if output2.size()[0] != 0:\n",
    "            if attention_mask[mask2] is not None:\n",
    "                active_loss2 = attention_mask[mask2].view(-1) == 1\n",
    "                #            print('active_loss2',active_loss2.size())\n",
    "                active_logits2 = output2.view(-1, TRAIN_CONFIG['num_labels2'])\n",
    "                #                 print('active_logits2',active_logits2.size())\n",
    "                active_labels2 = torch.where(\n",
    "                    active_loss2, target2.view(-1), torch.tensor(LOSS_FUNCTION.ignore_index).type_as(target2)\n",
    "                )\n",
    "                #                 print('active_labels2',active_labels2.size())\n",
    "                loss2 = LOSS_FUNCTION(active_logits2, active_labels2)\n",
    "            else:\n",
    "                loss2 = LOSS_FUNCTION(output2.view(-1, TRAIN_CONFIG['num_labels2']), target2.view(-1))\n",
    "        else:\n",
    "            loss2 = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "        #         print(loss1)\n",
    "        #        print(type(loss1))\n",
    "        return output1, output2, mask1, mask2, loss1, loss2\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# convert subtokens to tokens\n",
    "# for _,data in enumerate(testing_loader, start=0):\n",
    "# #         print(_)\n",
    "# #        print(data)\n",
    "#         for item in data['input_ids']:\n",
    "#             res=tokenizer.convert_ids_to_tokens(item)\n",
    "#             #print(res)\n",
    "#             res2=tokenizer.convert_tokens_to_string(res)\n",
    "#             res2=res2.replace('[PAD]','')\n",
    "# #            res2=res2.replace(' ',',')\n",
    "#             print(res2)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def train(model, epoch, cur_file, file_num, logging_step=1, training_loader=None, optimizer=None):\n",
    "    \"\"\"\n",
    "    model = train(model=model,\n",
    "              epoch=j,\n",
    "              cur_file=i+1,\n",
    "              file_num=file_num,\n",
    "              logging_step=logging_step,\n",
    "              training_loader=training_loader,\n",
    "              optimizer=optimizer)\n",
    "    \"\"\"\n",
    "    tr_loss1 = 0\n",
    "    tr_loss2 = 0\n",
    "    tr_total_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "\n",
    "    cnt = 0\n",
    "    for _, data in enumerate(training_loader, start=0):\n",
    "        \"\"\"\n",
    "        将training_loader::DataLoader里的数据用tensor的形式转到device上\n",
    "        \"\"\"\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        targets = data['labels'].to(device, dtype=torch.long)\n",
    "        classes = data['classes'].to(device, dtype=torch.long)\n",
    "\n",
    "        # 对梯度进行初始化,将梯度清零\n",
    "        # 每一次train的时候都会清零梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        output1, output2, mask1, mask2, loss1, loss2 = model(ids, attention_mask, targets, classes, device)\n",
    "\n",
    "        # 保存每个epoch最后一个file的结果\n",
    "        # if cur_file == file_num and cnt < 10:\n",
    "        #\n",
    "        #    cnt = cnt + 1\n",
    "        #    if _ == 0:\n",
    "        #        out_tensor1 = output1.clone()\n",
    "        #        out_tensor2 = output2.clone()\n",
    "        #        target_tensor1 = targets[mask1].clone()\n",
    "        #        target_tensor2 = targets[mask2].clone()\n",
    "        #    else:\n",
    "        #        out_tensor1 = torch.cat((out_tensor1, output1), 0)\n",
    "        #        out_tensor2 = torch.cat((out_tensor2, output2), 0)\n",
    "        #        target_tensor1 = torch.cat((target_tensor1, targets[mask1]), 0)\n",
    "        #        target_tensor2 = torch.cat((target_tensor2, targets[mask2]), 0)\n",
    "\n",
    "        # print(loss1.item(), loss2.item())\n",
    "\n",
    "        tr_loss1 += loss1.sum().item()\n",
    "        tr_loss2 += loss2.sum().item()\n",
    "        tr_total_loss += loss1.sum().item() + loss2.sum().item()\n",
    "\n",
    "        # 反向传播\n",
    "        # loss1.sum().backward(retain_graph=True)\n",
    "        # loss2.sum().backward()\n",
    "        loss = loss1.sum() + loss2.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += targets.size(0)\n",
    "\n",
    "        # logging\n",
    "        # 如果第n个batch能够整除logging_step，则打印loss\n",
    "        if _ % logging_step == 0:\n",
    "            loss_step = tr_total_loss / nb_tr_steps\n",
    "            loss_step1 = tr_loss1 / nb_tr_steps\n",
    "            loss_step2 = tr_loss2 / nb_tr_steps\n",
    "            # accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training Loss step{_}: total loss:{loss_step}, cap loss:{loss_step1}, res loss:{loss_step2}\")\n",
    "\n",
    "    # # evaluate\n",
    "    # if cur_file == file_num:\n",
    "    #     print('evaluate training result\\n')\n",
    "    #     with open('class_classification_report.txt', 'a') as f:\n",
    "    #         f.write(f'\\n~~~~~~~~~~training epoch{epoch} result:~~~~~~~~~~\\n')\n",
    "    #     true_labels1, true_predictions1 = compute_metrics(True, target_tensor1.cpu().data.numpy().tolist(),\n",
    "    #                                                       out_tensor1.argmax(-1).cpu().data.numpy().tolist(),\n",
    "    #                                                       'Capacitors')\n",
    "    #     true_labels2, true_predictions2 = compute_metrics(True, target_tensor2.cpu().data.numpy().tolist(),\n",
    "    #                                                       out_tensor2.argmax(-1).cpu().data.numpy().tolist(),\n",
    "    #                                                       'Resistors')\n",
    "\n",
    "    # print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    print(\"计算loss：\")\n",
    "    epoch_loss = tr_total_loss / nb_tr_steps\n",
    "    #    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    #    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = train(1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 计算 confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute(predictions, references, suffix=False):\n",
    "    report = classification_report(y_true=references, y_pred=predictions, suffix=suffix, output_dict=True)\n",
    "    report.pop(\"macro avg\")\n",
    "    report.pop(\"weighted avg\")\n",
    "    overall_score = report.pop(\"micro avg\")\n",
    "\n",
    "    scores = {\n",
    "        type_name: {\n",
    "            \"precision\": score[\"precision\"],\n",
    "            \"recall\": score[\"recall\"],\n",
    "            \"f1\": score[\"f1-score\"],\n",
    "            \"number\": score[\"support\"],\n",
    "        }\n",
    "        for type_name, score in report.items()\n",
    "    }\n",
    "    scores[\"overall_precision\"] = overall_score[\"precision\"]\n",
    "    scores[\"overall_recall\"] = overall_score[\"recall\"]\n",
    "    scores[\"overall_f1\"] = overall_score[\"f1-score\"]\n",
    "    scores[\"overall_accuracy\"] = accuracy_score(y_true=references, y_pred=predictions)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def compute_metrics(train_flag, labels, preds, classes):\n",
    "    #    print(labels)\n",
    "    # Remove ignored index (special tokens)\n",
    "    global all_class_list\n",
    "    true_predictions = [[attrDecoder(classes, p) for (p, l) in zip(prediction, label) if l != -100]\n",
    "                        for prediction, label in zip(preds, labels)]\n",
    "    true_labels = [[attrDecoder(classes, l) for (p, l) in zip(prediction, label) if l != -100]\n",
    "                   for prediction, label in zip(preds, labels)]\n",
    "\n",
    "    #     print(true_predictions)\n",
    "    #     print(true_labels)\n",
    "    results = compute(predictions=true_predictions, references=true_labels)\n",
    "    # print(results)\n",
    "\n",
    "    # 对result进行处理，以写入文件\n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, dict):\n",
    "            v['number'] = str(v['number'])\n",
    "\n",
    "    global write_result\n",
    "    print(f'write_result: {classes}')\n",
    "    if write_result:\n",
    "        #         file = open('class_classification_report.txt', 'w')\n",
    "        #         for k,v in dict.items():\n",
    "        #             file.write(k + ' ')\n",
    "        #             for k1, v1 in v.items():\n",
    "        #                 file.write(k1 + ' ' + str(v1) + ' ')\n",
    "        #             file.write(' \\n')\n",
    "        #         file.close()\n",
    "        #         class_preds = [attrDecoder(item) for item in true_predictions]\n",
    "        #         class_labels = [attrDecoder(item) for item in true_labels]\n",
    "        # class_report = classification_report(class_labels, class_preds,output_dict=True)\n",
    "        # class_report = classification_report(class_labels, class_preds)\n",
    "        #         with open('class_classification_report.txt','a') as f:\n",
    "        #             f.write(str(results))\n",
    "\n",
    "        with open('class_classification_report.json', 'a', errors='ignore') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        res = {\"precision\": results[\"overall_precision\"],\n",
    "               \"recall\": results[\"overall_recall\"],\n",
    "               \"f1\": results[\"overall_f1\"],\n",
    "               \"accuracy\": results[\"overall_accuracy\"]}\n",
    "        if train_flag:\n",
    "            print('train_result:', res)\n",
    "            with open('class_classification_report.txt', 'a') as f:\n",
    "                f.write(str(res) + '\\n')\n",
    "        else:\n",
    "            print('test_result:', res)\n",
    "            with open('class_classification_report.txt', 'a') as f:\n",
    "                f.write(str(res) + '\\n')\n",
    "\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "def test(model, logging_step, testing_loader, optimizer, test_class_part, sample_data):\n",
    "    model.eval()\n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    tr_loss1 = 0\n",
    "    tr_loss2 = 0\n",
    "    tr_total_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    with torch.no_grad():\n",
    "        cnt = 0\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            targets = data['labels'].to(device, dtype=torch.long)\n",
    "            classes = data['classes'].to(device, dtype=torch.long)\n",
    "            # print(classes.size())\n",
    "\n",
    "            # 前向传播\n",
    "            output1, output2, mask1, mask2, loss1, loss2 = model(ids, attention_mask, targets, classes, device)\n",
    "\n",
    "            if cnt < 1:\n",
    "                cnt = cnt + 1\n",
    "                if _ == 0:\n",
    "                    out_tensor1 = output1.clone()\n",
    "                    out_tensor2 = output2.clone()\n",
    "                    target_tensor1 = targets[mask1].clone()\n",
    "                    target_tensor2 = targets[mask2].clone()\n",
    "                else:\n",
    "                    out_tensor1 = torch.cat((out_tensor1, output1), 0)\n",
    "                    out_tensor2 = torch.cat((out_tensor2, output2), 0)\n",
    "                    target_tensor1 = torch.cat((target_tensor1, targets[mask1]), 0)\n",
    "                    target_tensor2 = torch.cat((target_tensor2, targets[mask2]), 0)\n",
    "\n",
    "            #             for item in ids[mask1]:\n",
    "            #                 res=tokenizer.convert_ids_to_tokens(item)\n",
    "            #                 #print(res)\n",
    "            #                 res=tokenizer.convert_tokens_to_string(res)\n",
    "            #                 res=res.replace('[PAD]','')\n",
    "            #                 input.append(res.strip())\n",
    "\n",
    "            # print(loss1.item(), loss2.item())\n",
    "            tr_loss1 += loss1.sum().item()\n",
    "            tr_loss2 += loss2.sum().item()\n",
    "            tr_total_loss += loss1.sum().item() + loss2.sum().item()\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            # logging\n",
    "            if _ % logging_step == 0:\n",
    "                loss_step = tr_total_loss / nb_tr_steps\n",
    "                loss_step1 = tr_loss1 / nb_tr_steps\n",
    "                loss_step2 = tr_loss2 / nb_tr_steps\n",
    "                # accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Testing Loss per step: total loss:{loss_step},cap loss:{loss_step1},res loss:{loss_step2}\")\n",
    "                # print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "    #     print(target_tensor1.size())\n",
    "    #     print(out_tensor1.argmax(-1).size())\n",
    "\n",
    "    with open('class_classification_report.txt', 'a') as f:\n",
    "        f.write('\\n~~~~~~~~~~testing result:~~~~~~~~~~\\n')\n",
    "    true_labels1, true_predictions1 = compute_metrics(False, target_tensor1.cpu().data.numpy().tolist(),\n",
    "                                                      out_tensor1.argmax(-1).cpu().data.numpy().tolist(), 'Capacitors')\n",
    "    true_labels2, true_predictions2 = compute_metrics(False, target_tensor2.cpu().data.numpy().tolist(),\n",
    "                                                      out_tensor2.argmax(-1).cpu().data.numpy().tolist(), 'Resistors')\n",
    "\n",
    "    cap_input = [v for i, v in zip(test_class_part, sample_data) if i == 'Capacitors']\n",
    "    res_input = [v for i, v in zip(test_class_part, sample_data) if i == 'Resistors']\n",
    "\n",
    "    min_len1 = min(len(true_labels1), len(cap_input))\n",
    "    min_len2 = min(len(true_labels2), len(res_input))\n",
    "\n",
    "    result_desc = cap_input[:min_len1] + res_input[:min_len2]\n",
    "    result_labels = true_labels1[:min_len1] + true_labels2[:min_len2]\n",
    "    result_predictions = true_predictions1[:min_len1] + true_predictions2[:min_len2]\n",
    "\n",
    "    join_original_input = pd.DataFrame(\n",
    "        {\"description\": result_desc, \"labels\": result_labels, 'predictions': result_predictions})\n",
    "    join_original_input.to_csv('test_description_result.csv', encoding='utf_8_sig')\n",
    "\n",
    "    #             if _%5000==0:\n",
    "    #                 loss_step = tr_loss/nb_tr_steps\n",
    "    #                 accu_step = (n_correct*100)/nb_tr_examples\n",
    "    #                 print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "    #                 print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    #     epoch_loss = tr_loss/nb_tr_steps\n",
    "    #     epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    #     print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    #     print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# test(model,1)\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def main(num_epoch=10, save_epoch=2, logging_step=10, file_num=10, train_size=150, test_size=30):\n",
    "    \"\"\"\n",
    "    logging_step:  每隔多少步打印一次结果\n",
    "    train_size:    是train_input_process的第一个参数\n",
    "    model: 就是一个 nerClass(TRAIN_CONFIG)\n",
    "    \"\"\"\n",
    "    model = nerClass(TRAIN_CONFIG)  # model\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': NUM_WORKERS}\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                   'shuffle': False,\n",
    "                   'num_workers': NUM_WORKERS}\n",
    "\n",
    "    # training loop\n",
    "    total_time_start = time.time()\n",
    "    for j in range(num_epoch):\n",
    "        print(f'\\n\\n%%%%%%%%%%%%%%epoch{j}%%%%%%%%%%%%%%')\n",
    "        for i in range(file_num):\n",
    "            print(f'\\n\\nEpoch{j} begin generate train_dataset{i}')\n",
    "            print('\\nProcessing Training data\\n')\n",
    "            # i, j = 0, 0\n",
    "            process_start_time = time.time()\n",
    "            train_data, train_labels_encoded, train_class = train_input_process(train_size=train_size,\n",
    "                                                                                tokenizer=tokenizer,\n",
    "                                                                                file_num=i)\n",
    "            # ClassEncoder，将 class 转化成了 Encoder 的形式\n",
    "            train_class_encoded = list(map(lambda x: ClassEncoder(x), train_class))\n",
    "            # 打包成一个DataLoader\n",
    "            train_dataset = processDataset(train_data, train_labels_encoded, train_class_encoded)\n",
    "            training_loader = DataLoader(train_dataset, **train_params)\n",
    "            process_end_time = time.time()\n",
    "            print(\"processing time: \", process_end_time - process_start_time)\n",
    "\n",
    "\n",
    "            # Train the model\n",
    "            print('\\nTraining Begins\\n')\n",
    "            train_start_time = time.time()\n",
    "            model = train(model=model,\n",
    "                          epoch=j,\n",
    "                          cur_file=i+1,\n",
    "                          file_num=file_num,\n",
    "                          logging_step=logging_step,\n",
    "                          training_loader=training_loader,\n",
    "                          optimizer=optimizer)\n",
    "            train_end_time = time.time()\n",
    "            print(f\"Training time: {train_end_time - train_start_time}\")\n",
    "            total_time_in_this_data = time.time()\n",
    "            print(f\"Training up to now using time: {total_time_in_this_data - total_time_start}\")\n",
    "            gc.collect()\n",
    "            print(\"This sub data file has finished! \\n\\n\")\n",
    "\n",
    "        if j % save_epoch == 0:\n",
    "            # 每隔 save_epoch 保存一次模型\n",
    "            output_model_file = f'./models/model{j}/model.bin'\n",
    "            output_vocab_file = f'./models/model{j}'\n",
    "            if not os.path.exists(f'./models/model{j}'):\n",
    "                os.makedirs(f'./models/model{j}')\n",
    "            torch.save(model, output_model_file)\n",
    "            tokenizer.save_pretrained(output_vocab_file)\n",
    "            print('All files saved')\n",
    "\n",
    "    total_time_end = time.time()\n",
    "    print('\\n\\nTotal_training_time:', total_time_start - total_time_end)\n",
    "\n",
    "\n",
    "\n",
    "    test_data, test_labels_encoded, test_class, sample_data, test_labels = test_input_process(test_size=test_size,\n",
    "                                                                                              tokenizer=tokenizer)\n",
    "    test_class_encoded = list(map(lambda x: ClassEncoder(x), test_class))\n",
    "    test_dataset = processDataset(test_data, test_labels_encoded, test_class_encoded)\n",
    "    testing_loader = DataLoader(test_dataset, **test_params)\n",
    "\n",
    "    # test process\n",
    "    test(model=model,\n",
    "         logging_step=logging_step,\n",
    "         testing_loader=testing_loader,\n",
    "         optimizer=optimizer,\n",
    "         test_class_part=test_class[:1000],\n",
    "         sample_data=sample_data)\n",
    "\n",
    "    # 保存最终模型\n",
    "    output_model_file = './models/model_final/model.bin'\n",
    "    output_vocab_file = './models/model_final'\n",
    "    if not os.path.exists('./models/model_final'):\n",
    "        os.makedirs('./models/model_final')\n",
    "    torch.save(model, output_model_file)\n",
    "    tokenizer.save_pretrained(output_vocab_file)\n",
    "    print('All files saved')\n",
    "\n",
    "\n",
    "# In[24]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The code is running on my PC!\n",
      "\n",
      "The calculating device is:  cpu\n",
      "\n",
      "超参数为： {'LEARNING_RATE': 1e-05, 'TRAIN_BATCH_SIZE': 4, 'VALID_BATCH_SIZE': 4, 'NUM_WORKERS': 6}\n",
      "\n",
      "主函数参数为： {'num_epoch': 1, 'save_epoch': 1, 'logging_step': 4, 'file_num': 5, 'train_size': 16, 'test_size': 16}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 注意最开始的label处理部分，已经把没意义的赋值为-100了\n",
    "    with open(\"config.json\", 'r') as f:\n",
    "        config_raw = f.read()\n",
    "        config_raw = json.loads(config_raw)\n",
    "\n",
    "    project_path = os.getcwd()\n",
    "    if project_path[:12] == '/Users/meron':\n",
    "        config = config_raw[\"PC\"]\n",
    "        print(\"\\nThe code is running on my PC!\")\n",
    "    else:\n",
    "        config = config_raw[\"Server\"]\n",
    "        print(\"\\n The code is running on the Server!\")\n",
    "\n",
    "\n",
    "\n",
    "    cap_attrs = config[\"encoder\"][\"cap_attrs\"]\n",
    "    res_attrs = config[\"encoder\"][\"res_attrs\"]\n",
    "    all_class_list = {'Capacitors': cap_attrs, 'Resistors': res_attrs}\n",
    "\n",
    "    # 将不同的category映射到一个int上\n",
    "    cap_attrs_dict = dict(zip(cap_attrs.values(), cap_attrs.keys()))\n",
    "    res_attrs_dict = dict(zip(res_attrs.values(), res_attrs.keys()))\n",
    "    all_attrs_dict = {**cap_attrs_dict, **res_attrs_dict}\n",
    "\n",
    "    write_result = True\n",
    "    set_seed(1024)\n",
    "\n",
    "\n",
    "    CHECK_POINT = config[\"train_config\"][\"model\"]\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECK_POINT)\n",
    "\n",
    "    print('\\nThe calculating device is: ', device)\n",
    "\n",
    "    num_labels1 = config[\"train_config\"][\"num_labels1\"]    # 修改成CAP或RES\n",
    "    num_labels2 = config[\"train_config\"][\"num_labels2\"]\n",
    "    train_model = config[\"train_config\"][\"model\"]\n",
    "    TRAIN_CONFIG = {'num_labels1': num_labels1,\n",
    "                    'num_labels2': num_labels2,\n",
    "                    'model': train_model}\n",
    "\n",
    "    ADDRESS = config[\"address\"]\n",
    "\n",
    "    Hyperparameter = config[\"Hyperparameter\"]\n",
    "    LEARNING_RATE = Hyperparameter[\"LEARNING_RATE\"]\n",
    "    TRAIN_BATCH_SIZE = Hyperparameter[\"TRAIN_BATCH_SIZE\"]\n",
    "    VALID_BATCH_SIZE = Hyperparameter[\"VALID_BATCH_SIZE\"]\n",
    "    NUM_WORKERS = Hyperparameter[\"NUM_WORKERS\"]\n",
    "    LOSS_FUNCTION = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    print(\"\\n超参数为：\", Hyperparameter)\n",
    "\n",
    "    Main_Parameter = config[\"Main_Parameter\"]\n",
    "    num_epoch = Main_Parameter[\"num_epoch\"]\n",
    "    save_epoch = Main_Parameter[\"save_epoch\"]\n",
    "    logging_step = Main_Parameter[\"logging_step\"]\n",
    "    file_num = Main_Parameter[\"file_num\"]\n",
    "    train_size = Main_Parameter[\"train_size\"]\n",
    "    test_size = Main_Parameter[\"test_size\"]\n",
    "    print(\"\\n主函数参数为：\", Main_Parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "    # main(num_epoch, save_epoch, logging_step, file_num, train_size, test_size)\n",
    "    # main(num_epoch=100, save_epoch=1, logging_step=100, file_num=100, train_size=10000, test_size=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 进入 mian() 循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_params\n",
      " {'batch_size': 4, 'shuffle': False, 'num_workers': 6}\n",
      "\n",
      "test_params\n",
      " {'batch_size': 4, 'shuffle': False, 'num_workers': 6}\n",
      "\n",
      " nerClass(\n",
      "  (l1): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier1): Linear(in_features=128, out_features=9, bias=True)\n",
      "  (classifier2): Linear(in_features=128, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nerClass(TRAIN_CONFIG)  # model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': NUM_WORKERS}\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "               'shuffle': False,\n",
    "               'num_workers': NUM_WORKERS}\n",
    "\n",
    "# training loop\n",
    "total_time_start = time.time()\n",
    "\n",
    "print(\"\\ntrain_params\\n\", train_params)\n",
    "print(\"\\ntest_params\\n\", test_params)\n",
    "print('\\n', model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset0\n",
      "\n",
      "read train dataset0\n",
      "\n",
      "training dataset is ok\n",
      "\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "input_ids\n",
      " tensor([[22238,  2620, 14376,  ...,     0,     0,     0],\n",
      "        [ 4293,  1003,  3118,  ...,     0,     0,     0],\n",
      "        [ 1031,  1005,  4860,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 1031,  1005,  6421,  ...,     0,     0,     0],\n",
      "        [ 3429,  2475,  2290,  ...,     0,     0,     0],\n",
      "        [ 1054,  2692, 17914,  ...,     0,     0,     0]])\n",
      "\n",
      "train_data_data_shape\n",
      " torch.Size([160, 148])\n",
      "\n",
      "There is 160 data, and length of each data is 148.\n",
      "\n",
      "\n",
      "token_type_ids\n",
      " tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "\n",
      "attention_mask\n",
      " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "\n",
      "tokens_length\n",
      " 148\n",
      "\n",
      "tokens\n",
      " ['228', '##8', '##pf', '[', \"'\", '-', '[UNK]', \"'\", ',', \"'\", 'aluminum', '(', 'wet', ')', \"'\", ',', \"'\", '228', '##8', '##pf', \"'\", ',', \"'\", '93', '##8', '##3', '##m', \"'\", ']', '-', '[UNK]', 'aluminum', '(', 'wet', ')', '93', '##8', '##3', '##m', '[', \"'\", 'operating', '##tem', '##per', '##at', '##ure', '##min', \"'\", ',', \"'\", 'die', '##le', '##ctric', '##mate', '##rial', \"'\", ',', \"'\", 'cap', '##ac', '##itan', '##ce', \"'\", ',', \"'\", 'rated', '##dc', '##vo', '##lta', '##ge', '##ur', '##dc', \"'\", ']', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "i, j = 0, 0\n",
    "train_data, train_labels_encoded, train_class = train_input_process(train_size=train_size,\n",
    "                                                                    tokenizer=tokenizer,\n",
    "                                                                    file_num=i)\n",
    "# train_data.__dir__()\n",
    "print(train_data.keys())\n",
    "print('\\ninput_ids\\n',             train_data.data['input_ids'])\n",
    "print('\\ntrain_data_data_shape\\n', train_data.data['input_ids'].shape)\n",
    "\n",
    "data_shape = train_data.data['input_ids'].shape\n",
    "print('\\nThere is {} data, and length of each data is {}.\\n'.format(data_shape[0], data_shape[1]))\n",
    "print('\\ntoken_type_ids\\n',        train_data.data['token_type_ids'])\n",
    "print('\\nattention_mask\\n',        train_data.data['attention_mask'])\n",
    "\n",
    "# train_data.values()\n",
    "# train_data.encodings\n",
    "print('\\ntokens_length\\n',                len(train_data.tokens()))\n",
    "print('\\ntokens\\n',                       train_data.tokens()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset0\n",
      "\n",
      "read train dataset0\n",
      "\n",
      "training dataset is ok\n",
      "\n",
      "length of data 0 148 [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 1 148 [4, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 2 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 3 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 3, 3, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 4 148 [2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 5 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 4, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 6 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 1, 1, 1, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 7 148 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 4, 4, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 8 148 [8, 8, 8, 8, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "length of data 9 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "length of train_calss\n",
      " 160\n",
      "['Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Capacitors', 'Resistors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors', 'Resistors', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Resistors']\n"
     ]
    }
   ],
   "source": [
    "i, j = 0, 0\n",
    "train_data, train_labels_encoded, train_class = train_input_process(train_size=train_size,\n",
    "                                                                    tokenizer=tokenizer,\n",
    "                                                                    file_num=i)\n",
    "for i in range(10):\n",
    "    print(f'length of data {i}', len(train_labels_encoded[i]), train_labels_encoded[i])\n",
    "\n",
    "print('\\nlength of train_calss\\n', len(train_class))\n",
    "print(train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# ClassEncoder，将 class 转化成了 Encoder 的形式\n",
    "train_class_encoded = list(map(lambda x: ClassEncoder(x), train_class))\n",
    "print(train_class_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 打包成一个DataLoader\n",
    "train_dataset = processDataset(train_data, train_labels_encoded, train_class_encoded)\n",
    "training_loader = DataLoader(train_dataset, **train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(training_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for _, data in enumerate(training_loader, start=0):\n",
    "#     ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#     attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#     targets = data['labels'].to(device, dtype=torch.long)\n",
    "#     classes = data['classes'].to(device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 查看 NERClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_labels1': 9, 'num_labels2': 8, 'model': 'prajjwal1/bert-tiny'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method nerClass.forward of nerClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier1): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (classifier2): Linear(in_features=128, out_features=8, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class nerClass(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    定义ner的Class，是torch里面的一个层\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, TRAIN_CONFIG):\n",
    "        \"\"\"\n",
    "        TRAIN_CONFIG 是个全局变量     config = {'num_labels1': 9, 'num_labels2': 8, 'model': \"prajjwal1/bert-tiny\"}\n",
    "                              config['num_labels1] = 9   分成9类\n",
    "                              config['num_labels1] = 8   分成8类\n",
    "                              config['model'] = \"prajjwal1/bert-tiny\" 预训练的模型\n",
    "        \"\"\"\n",
    "        super(nerClass, self).__init__()\n",
    "        print(TRAIN_CONFIG)\n",
    "        if TRAIN_CONFIG[\"model\"] == \"prajjwal1/bert-tiny\":\n",
    "            self.embedding_length = 128\n",
    "        elif TRAIN_CONFIG[\"model\"] == \"albert-base-v2\":\n",
    "            self.embedding_length = 768\n",
    "        self.num_labels1 = TRAIN_CONFIG['num_labels1']\n",
    "        self.num_labels2 = TRAIN_CONFIG['num_labels2']\n",
    "        self.l1 = BertModel.from_pretrained(TRAIN_CONFIG['model'])\n",
    "\n",
    "        #         self.pre_classifier = torch.nn.Linear(128, 128)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier1 = torch.nn.Linear(self.embedding_length, TRAIN_CONFIG['num_labels1'])  # classifier1是一个线性分类器\n",
    "        self.classifier2 = torch.nn.Linear(self.embedding_length, TRAIN_CONFIG['num_labels2'])\n",
    "\n",
    "    #        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, targets=None, classes=None, device=None):\n",
    "        #         output_1 = self.l1(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
    "        #                             attention_mask=attention_mask, head_mask=head_mask)\n",
    "        # the hidden states of the last Bert Layer, shape: (bsz, seq_len, hsz)\n",
    "\n",
    "        \"\"\"\n",
    "        input_ids:\n",
    "        attention_mask:\n",
    "        targets:\n",
    "        classes:\n",
    "        device:\n",
    "        \"\"\"\n",
    "        output = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]  # 输出的第0个return\n",
    "        # last_hidden_state (torch.FloatTensor of shape (\n",
    "        #                    batch_size, sequence_length, hidden_size))\n",
    "        # print('hidden_state.size()',hidden_state[:,0].size())\n",
    "        # pooler = self.pre_classifier(pooler)\n",
    "        # pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(hidden_state)\n",
    "        print('pooler.size()',pooler.size())\n",
    "\n",
    "        mask1 = torch.eq(classes, 1)  # classes 是输入，判断是否等于 1\n",
    "        mask2 = torch.eq(classes, 2)\n",
    "        print('pooler[mask1].size()', pooler[mask1].size())\n",
    "        print('pooler[mask2].size()', pooler[mask2].size())\n",
    "\n",
    "        # 这里是两个头，分开进行预测\n",
    "        output1 = self.classifier1(pooler[mask1])  # 是pooler经过mask1筛选后的\n",
    "        output2 = self.classifier2(pooler[mask2])  # 是pooler经过mask2筛选后的\n",
    "        print('output1.size()',output1.size())\n",
    "        print('output2.size()',output2.size())\n",
    "\n",
    "        #        output1 = output1[mask1]\n",
    "        target1 = targets[mask1]  # targets是一个input的参数\n",
    "        print('target1.size()',target1.size())\n",
    "\n",
    "        #        output2 = output2[mask2]\n",
    "        target2 = targets[mask2]\n",
    "        print('target2.size()',target2.size())\n",
    "\n",
    "        if output1.size()[0] != 0:\n",
    "            if attention_mask[mask1] is not None:\n",
    "                \"\"\"\n",
    "                # loss_function 是 CrossEntropy, 忽略 -100\n",
    "                \"\"\"\n",
    "                active_loss1 = attention_mask[mask1].view(-1) == 1\n",
    "                #            print('active_loss1',active_loss1.size())\n",
    "                active_logits1 = output1.view(-1, TRAIN_CONFIG['num_labels1'])\n",
    "                #                 print('active_logits1',active_logits1.size())\n",
    "                active_labels1 = torch.where(\n",
    "                    active_loss1, target1.view(-1), torch.tensor(LOSS_FUNCTION.ignore_index).type_as(target1)\n",
    "                )\n",
    "                #                 print('active_labels1',active_labels1.size())\n",
    "                loss1 = LOSS_FUNCTION(active_logits1, active_labels1)\n",
    "            else:\n",
    "                loss1 = LOSS_FUNCTION(output1.view(-1, TRAIN_CONFIG['num_labels1']), target1.view(-1))\n",
    "        else:\n",
    "            # 如果batch_size是0的话，loss1的值就是0\n",
    "            loss1 = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "\n",
    "        if output2.size()[0] != 0:\n",
    "            if attention_mask[mask2] is not None:\n",
    "                active_loss2 = attention_mask[mask2].view(-1) == 1\n",
    "                #            print('active_loss2',active_loss2.size())\n",
    "                active_logits2 = output2.view(-1, TRAIN_CONFIG['num_labels2'])\n",
    "                #                 print('active_logits2',active_logits2.size())\n",
    "                active_labels2 = torch.where(\n",
    "                    active_loss2, target2.view(-1), torch.tensor(LOSS_FUNCTION.ignore_index).type_as(target2)\n",
    "                )\n",
    "                #                 print('active_labels2',active_labels2.size())\n",
    "                loss2 = LOSS_FUNCTION(active_logits2, active_labels2)\n",
    "            else:\n",
    "                loss2 = LOSS_FUNCTION(output2.view(-1, TRAIN_CONFIG['num_labels2']), target2.view(-1))\n",
    "        else:\n",
    "            loss2 = torch.tensor(0.0, requires_grad=True, device=device)\n",
    "        #         print(loss1)\n",
    "        #        print(type(loss1))\n",
    "        return output1, output2, mask1, mask2, loss1, loss2\n",
    "\n",
    "model = nerClass(TRAIN_CONFIG)\n",
    "model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'processDataset' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5b177db88b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'processDataset' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tr_loss1 = 0\n",
    "tr_loss2 = 0\n",
    "tr_total_loss = 0\n",
    "n_correct = 0\n",
    "nb_tr_steps = 0\n",
    "nb_tr_examples = 0\n",
    "model.train()\n",
    "\n",
    "cnt = 0\n",
    "for _, data in enumerate(training_loader, start=0):\n",
    "    \"\"\"\n",
    "    将training_loader::DataLoader里的数据用tensor的形式转到device上\n",
    "    \"\"\"\n",
    "    ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "    attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "    targets = data['labels'].to(device, dtype=torch.long)\n",
    "    classes = data['classes'].to(device, dtype=torch.long)\n",
    "\n",
    "    # 对梯度进行初始化,将梯度清零\n",
    "    # 每一次train的时候都会清零梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 前向传播\n",
    "    output1, output2, mask1, mask2, loss1, loss2 = model(ids, attention_mask, targets, classes, device)\n",
    "\n",
    "    # 保存每个epoch最后一个file的结果\n",
    "    # if cur_file == file_num and cnt < 10:\n",
    "    #\n",
    "    #    cnt = cnt + 1\n",
    "    #    if _ == 0:\n",
    "    #        out_tensor1 = output1.clone()\n",
    "    #        out_tensor2 = output2.clone()\n",
    "    #        target_tensor1 = targets[mask1].clone()\n",
    "    #        target_tensor2 = targets[mask2].clone()\n",
    "    #    else:\n",
    "    #        out_tensor1 = torch.cat((out_tensor1, output1), 0)\n",
    "    #        out_tensor2 = torch.cat((out_tensor2, output2), 0)\n",
    "    #        target_tensor1 = torch.cat((target_tensor1, targets[mask1]), 0)\n",
    "    #        target_tensor2 = torch.cat((target_tensor2, targets[mask2]), 0)\n",
    "\n",
    "    # print(loss1.item(), loss2.item())\n",
    "\n",
    "    tr_loss1 += loss1.sum().item()\n",
    "    tr_loss2 += loss2.sum().item()\n",
    "    tr_total_loss += loss1.sum().item() + loss2.sum().item()\n",
    "\n",
    "    # 反向传播\n",
    "    # loss1.sum().backward(retain_graph=True)\n",
    "    # loss2.sum().backward()\n",
    "    loss = loss1.sum() + loss2.sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    nb_tr_steps += 1\n",
    "    nb_tr_examples += targets.size(0)\n",
    "\n",
    "    # logging\n",
    "    # 如果第n个batch能够整除logging_step，则打印loss\n",
    "    if _ % logging_step == 0:\n",
    "        loss_step = tr_total_loss / nb_tr_steps\n",
    "        loss_step1 = tr_loss1 / nb_tr_steps\n",
    "        loss_step2 = tr_loss2 / nb_tr_steps\n",
    "        # accu_step = (n_correct*100)/nb_tr_examples\n",
    "        print(f\"Training Loss step{_}: total loss:{loss_step}, cap loss:{loss_step1}, res loss:{loss_step2}\")\n",
    "\n",
    "# # evaluate\n",
    "# if cur_file == file_num:\n",
    "#     print('evaluate training result\\n')\n",
    "#     with open('class_classification_report.txt', 'a') as f:\n",
    "#         f.write(f'\\n~~~~~~~~~~training epoch{epoch} result:~~~~~~~~~~\\n')\n",
    "#     true_labels1, true_predictions1 = compute_metrics(True, target_tensor1.cpu().data.numpy().tolist(),\n",
    "#                                                       out_tensor1.argmax(-1).cpu().data.numpy().tolist(),\n",
    "#                                                       'Capacitors')\n",
    "#     true_labels2, true_predictions2 = compute_metrics(True, target_tensor2.cpu().data.numpy().tolist(),\n",
    "#                                                       out_tensor2.argmax(-1).cpu().data.numpy().tolist(),\n",
    "#                                                       'Resistors')\n",
    "\n",
    "# print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "print(\"计算loss：\")\n",
    "epoch_loss = tr_total_loss / nb_tr_steps\n",
    "#    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "#    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
