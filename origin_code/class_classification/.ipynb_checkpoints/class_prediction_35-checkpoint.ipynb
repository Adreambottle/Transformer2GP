{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampling\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json\n",
    "from simpletransformers.ner import NERModel,NERArgs\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class_features = sampling.read_data(\"config/config-class-features.json\")\n",
    "config_class_name = sampling.read_data(\"config/config-class-name.json\")\n",
    "config_classinfo = sampling.read_data(\"config/config-classinfo.json\")\n",
    "config_numeric_fields = sampling.read_data(\"config/config-numeric-fields.json\")\n",
    "config_dynamic_units = sampling.read_data(\"config/config-dynamic-units.json\")\n",
    "pair_params = sampling.read_data(\"config/pair-params\")\n",
    "with open('class_dict.txt', 'r') as f:\n",
    "    js = f.read()\n",
    "    class_dict = json.loads(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_flag True\n",
      "problematic set()\n",
      "query数量： 84\n",
      "label数量： 84\n",
      "[' Circuit Protection 55.0uV 10.025nA LONG 240.0V e4 TRIP FREE TEB', ' 电容 1206 0.025µF CHASSIS MOUNT C0G MIL-PRF-55681 YES', ' 连接器 CIRCULAR CONNECTOR ADAPTER EMI SHIELDED, HIGH DENSITY 4 ALUMINUM PANEL', ' WRAPAROUND 3.5% CERAMIC 2/2µF Capacitors 900.0 20.5kV', ' 6.0ppm/°C -55.0°C 125.0°C 82.0kΩ METAL GLAZE/THICK FILM BUSSED 38415.0Ohm 1.05W 50ppm/°C', ' 11.965mm SIGNAL 8000 OHMS CT 15.0kHz THROUGH HOLE MOUNTED 0.35kHz 3/10', ' SMA FEMALE 1.5 5 50 OHM 7.0', ' 1/8°C YES 8.75 62.5uV C0G cap 9/9µF 8/5°C CERAMIC', ' WRAPAROUND e4 325.0V 0.003µF 1812', ' MECHANICAL TUNED CAVITY OSCILLATOR 10.0kHz -13.5 -20.0uV 2000.0% -20.0', ' 0.78mm X7R 0.005µF W1A 12.5% YES', ' 5.125kV 2.0µs 6/6°C GULL WING 4.0V INDUSTRIAL 2.5 70.0°C Converters', ' Res SPD16B2002CL 50ppm/°C MIL-PRF-83401 0.24mW Array/Network Resistors 6.74K THIN FILM Resistors', ' 3.79 0.001µF 4.5mm X7R CERAMIC SILVER PALLADIUM 10.0% Yes', ' 9/3µF 0.1 NO 10/6mΩ 52.5V 7.65', ' TR, 7 INCH 电容 0.002µF Y5V WRAPAROUND CERAMIC', ' 8.0 FLASH Microcontrollers and Processors M8C 4/3', ' 2125.0mA 7/8mΩ 75.135µF POLARIZED EKE', ' 44.0 e3/e4 13.995MHz Crystals/Resonators 0.3 5 PPM/YEAR 5.005MHz', ' CERAMIC 5/8µF 10 33.0kV 6/5 SURFACE MOUNT CS201 C0G', ' SURFACE MOUNT 7.51mm 7/4mm SPD08A METAL GLAZE/THICK FILM 6860.0ohm 1.15mW 100ppm/°C', ' 50.0mV cap RECTANGULAR PACKAGE MA6X1C223MYDG 1.3 15.0% 0.018µF 9 CERAMIC', ' 51840.0 -5.2 5.715mm STS-1/OC-1 ATM/SONET/SDH ICs CMOS 4.15uV', ' 6/3mm C0G SURFACE MOUNT 15.0% 1210 MIL-PRF-55681/9 1.7e-06µF', ' 25.85µF THROUGH HOLE MOUNT TANTALUM (DRY/SOLID) 20.0% 40.0mΩ CYLINDRICAL PACKAGE', ' IN-LINE GULL WING 7/9 N/A 40.0 4.15kV', ' Drivers And Interfaces 1 5.0 3/6 GULL WING 13.5', ' 200.0V RECTANGULAR PACKAGE 0.001µF C0G 1/3°C SURFACE MOUNT -55.0°C Other Cap.Values & Voltage Available 5.5%', ' Array/Network Capacitors 5/4% 3.413µF 14.335mm 0805', ' 4/7% ALUMINUM (WET) 5000.235µF 电容 e0 2', ' Y5V YES CERAMIC METALLIZED FILM 0.005µF CA052X392M3RAL Cap', ' Not Qualified 4/3 YES 5.5 9/10', ' 1.285 PANEL 41 MULTIPLE MATING PARTS AVAILABLE 61 e4', ' Trigger Devices 75.0 5/9 3/8 Breakover Diodes RECTANGULAR 4/3mA 229.5V', ' CERAMIC NCA 0.044µF 50.0V 125.0°C 1/10°C', ' BOARD 6/4 LOCKING 9 e4', ' 0402 RECTANGULAR PACKAGE 7/10 C0G CERAMIC Other Cap. & Volt. Available On Request 5/3µF', ' AEC-Q200 0.0µF ARRAY/NETWORK CAPACITOR e2 7.5% X7R 1206', ' BULK ALUMINUM (WET) 11004.1µF 123SAL-A ALUMINUM ELECTROLYTIC CAPACITOR 3939 POLARIZED 10.0%', ' CERAMIC CAPACITOR YES CERAMIC C0G 3/1 9/10e-06µF', ' 0.84nW RECTANGULAR PACKAGE 14 THIN FILM 3.5 电阻 3575.0GΩ', ' Silver/Palladium (Ag/Pd) X7R 1.94mm MIL-PRF-55681 CERAMIC BULK 3.885µF 62.5mV', ' 105.0°C DIP 100.0 3.3 8/10GHz 9/9°C 5.0V', ' CERAMIC 3/9mm 8/7e-05µF Yes 10/10% TR, EMBOSSED/PAPER, 13 INCH X7R', ' 3807 1.0 LOW SKEW CLOCK DRIVER GULL WING 0.012A POSITIVE EDGE Clock Drivers 50.0', ' Gold (Au) 10.0% 1206 0.5e-06µF CERAMIC MA6Y1H223ZBFN BUSSED C NETWORK', ' 6335.0R 3051 0.075uW 10.0 5000.0 METAL GLAZE/THICK FILM Array/Network Resistors', ' 12.0 DUAL 5.0V IDM2901A1DM/883B 33.3', ' BIDIRECTIONAL Logic 1.0 Tin/Lead (Sn/Pb) TUBE 500k Rad(Si) OPEN-EMITTER 5.0V', ' -55.0°C 12.39mΩ 5.0ppm/°C 125.0°C 0.75kW 1606 0.525% 17400.0', ' BINDING POST 1/2µF 7/10mΩ Snap-in 电容 875.0mA 20.0%', ' BICMOS 10/4 Telecommunication Circuits 6.503 5.0V LDCC32,.5X.6', ' 2.0mV 0.532Ah 10/5 TRAY 3/4mA', ' 3.3 425.0 85.0°C 1.0 Telecommunication Circuits CMOS 2/10°C 1.0', ' ALUMINUM 5/3µF Aluminum Electrolytic Capacitors 51.5kV 95.0°C 5630 BULK 222204133478 -47.5°C', ' 1.1µF 7.5% CHIP 15.0% 2.25mm 1206 X7R 8', ' 1000.35V/us 7/1mA BUFFER 3/1µA 10.0V 300k Rad(Si) 1.0 95.0°C -47.5°C', ' POLARIZED 6/3mΩ -32.5°C 9/10 Matte Tin (Sn) NO 85.0°C 2/10µF TANTALUM (DRY/SOLID)', ' YES GULL WING Automotive Circuits 12.75 12V', ' Aluminum Electrolytic Capacitors TUBULAR PACKAGE 电容 ALUMINUM (WET) 49.0V 610.0µF 363.5mΩ 7/3% THROUGH HOLE MOUNT', ' 6765.0GΩ Array/Network Resistors 0.525MW 150.0V YES 100ppm/°C 6.705mm THIN FILM 1.203mm', ' 10/10mΩ SNAP-IN 电容 8/9µF 0.08 2626 REFERENCE STANDARD: IEC60384-4 BULK NON-POLARIZED', ' BUTTERWORTH 12.0mV 14 LOWPASS 1700.0 2.0 NO 6/10mm', ' 0805 1.77mm 15.0% 75.0V 0.009µF THROUGH HOLE MOUNT ARRAY/NETWORK CAPACITOR', ' DIE 9/6 HAND-HELD DEVICES Audio Synthesizer ICs', ' 2 1458052400829+ 40.0 0.65 Headers and Edge Type Connectors NOT SPECIFIED RECTANGULAR BOARD', ' 2.352µF CERAMIC Capacitors e4 6/4% VJ OMD 125.0uV 1.395mm', ' 1.0MHz 14 3.6 1.0 NO 15.0uV 10/3°C NOT SPECIFIED', ' 21.0 CRIMP 5/5 PROTECTIVE COVER MACH OR LOOSE 200.0 5/1°C 340KS002G32-5F6E -65.0°C', ' 2.5 2540 12863.0Ohm Array/Network Resistors 183060.0Ohm METAL GLAZE/THICK FILM 0.725nW TR', ' ALUMINUM (WET) 50.195µF 222202431151 3939 IEC60384-4 10/1', ' 1.0e-05 13.25uA 0.35 R-PDSO-G4 NOT SPECIFIED diodes SILICON AVALANCHE', ' VJ0402Y472KNAAO2L Other Cap. & Volt. Available On Request 2220 CERAMIC Tin/Lead (Sn/Pb) - with Nickel (Ni) barrier 0.003µF', ' 5/3µF 2 1.65mm 10.5% SMT CERAMIC', ' SMA FEMALE 7/5 3FV30-FREQ/TBW2-OB/OB 6/3MHz PANEL MOUNT N MALE L42.69XB14.42XH12.7 (mm)/L1.681XB0.568XH0.5 (inch)', ' 0.165µF Tin (Sn) - with Nickel (Ni) barrier CERAMIC 1.74mm Capacitors 0805 5.5%', ' 35.0% -47.5°C THROUGH HOLE MOUNT 9/2°C 2510.0mΩ BULK 4.5µF', ' POLARIZED 27.0mΩ 601D109F016HP2E3 136.95µF ALUMINUM ELECTROLYTIC CAPACITOR', ' 7.0 Yes CERAMIC C0G 1.1e-05µF', ' 33.0 3 e0 0.501% 1.5', ' POWER 115.0V EB 9/8 230', ' CA064C822M8RACAUTO 125.0°C 50.0kV 15.0% 1.0 0.001µF Y5V', ' res 0.5% 2950.0GΩ Conformal Coated THIN FILM 4.553mm 6.385mm', ' 165.165µF TUBULAR PACKAGE Aluminum Electrolytic Capacitors 7.95 POLARIZED 3939']\n",
      "['Circuit Protection', 'Capacitors', 'Connectors', 'Capacitors', 'Resistors', 'Transformers', 'Filters', 'Capacitors', 'Capacitors', 'Oscillators', 'Capacitors', 'Converters', 'Resistors', 'Capacitors', 'Capacitors', 'Capacitors', 'Microcontrollers and Processors', 'Capacitors', 'Crystals/Resonators', 'Capacitors', 'Resistors', 'Capacitors', 'Telecommunication Circuits', 'Capacitors', 'Capacitors', 'Logic', 'Drivers And Interfaces', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Consumer Circuits', 'Connectors', 'Trigger Devices', 'Capacitors', 'Connectors', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Resistors', 'Capacitors', 'Signal Circuits', 'Capacitors', 'Logic', 'Capacitors', 'Resistors', 'Microcontrollers and Processors', 'Logic', 'Resistors', 'Capacitors', 'Telecommunication Circuits', 'Batteries', 'Telecommunication Circuits', 'Capacitors', 'Capacitors', 'Amplifier Circuits', 'Capacitors', 'Signal Circuits', 'Capacitors', 'Resistors', 'Capacitors', 'Filters', 'Capacitors', 'Consumer Circuits', 'Connectors', 'Capacitors', 'Signal Circuits', 'Connector Support', 'Resistors', 'Capacitors', 'Diodes', 'Capacitors', 'Capacitors', 'Filters', 'Capacitors', 'Capacitors', 'Capacitors', 'Capacitors', 'Consumer Circuits', 'Transformers', 'Capacitors', 'Resistors', 'Capacitors']\n"
     ]
    }
   ],
   "source": [
    "#19个resistors的文件,12个capacitors的文件，总共374个文件\n",
    "def get_rs_list_of_all_category():\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "    # 读取所有catogory文件\n",
    "    path = os.getcwd()\n",
    "    files= os.listdir('./formatData') #得到文件夹下的所有文件名称\n",
    "    rs_list = []\n",
    "    \n",
    "    for file in files: #遍历文件夹\n",
    "        file_path = os.path.join(path, 'formatData/'+file)\n",
    "        if os.path.isfile(file_path): #判断是否是文件夹，不是文件夹才打开\n",
    "            if 'Resistors' in file:\n",
    "                for i in range(9):\n",
    "                    rs = sampling.sampling(file,0.6)\n",
    "                    rs_list.append((rs,file))\n",
    "            elif 'Capacitors' in file:\n",
    "                for i in range(14):\n",
    "                    rs = sampling.sampling(file,0.6)\n",
    "                    rs_list.append((rs,file))\n",
    "            else:\n",
    "                rs = sampling.sampling(file,0.6)\n",
    "                rs_list.append((rs,file))\n",
    "    \n",
    "    random.shuffle(rs_list)        \n",
    "    return rs_list\n",
    "\n",
    "def get_input_from_sampling(input_size: int,train_flag:True):\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "    rs_list = get_rs_list_of_all_category()\n",
    "    \n",
    "    # input_size为1表示从每一个category抽取一个样本\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    problematic = set()\n",
    "    \n",
    "    for i in range(input_size):\n",
    "        for rs in rs_list:\n",
    "            try:\n",
    "                item=rs[0].random_sampling()\n",
    "            except Exception as e:\n",
    "                problematic.add(rs[1])\n",
    "                continue\n",
    "                \n",
    "            # 保存抽样数据\n",
    "            if i<5 and train_flag:\n",
    "                with open('input_saved.txt','a') as f: \n",
    "                    js = json.dumps(item) \n",
    "                    f.write(js)\n",
    "            \n",
    "            tmp_string = ''\n",
    "            \n",
    "            shuffled_item = list(item.items())\n",
    "            random.shuffle(shuffled_item)\n",
    "            for key, val in shuffled_item:\n",
    "                if key == 'category':\n",
    "                    continue\n",
    "                elif key == 'class':\n",
    "                    label_list.append(val)\n",
    "                else:\n",
    "                    tmp_string+=' '+str(val)\n",
    "\n",
    "            text_list.append(tmp_string)\n",
    "    \n",
    "    print('train_flag',train_flag)\n",
    "    print('problematic',problematic)\n",
    "    print('query数量：',len(text_list))\n",
    "    print('label数量：',len(label_list))\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('train_flag:'+str(train_flag)+'\\n')\n",
    "        f.write('query数量:'+str(len(text_list))+'\\n')\n",
    "        f.write('label数量:'+str(len(label_list))+'\\n')\n",
    "        f.write('problematic:'+str(problematic)+'\\n')\n",
    "    return text_list,label_list\n",
    "\n",
    "# a,b=get_input_from_sampling(1,True)\n",
    "# print(a)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_process(config_classinfo,model_checkpoint,tokenizer,train_size,test_size,class_dict):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    files= os.listdir('./class_inputData')\n",
    "\n",
    "    with open('running_output.txt','w') as f:\n",
    "        f.write(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+'\\n')\n",
    "    \n",
    "    #准备训练数据\n",
    "    print('train dataset\\n')\n",
    "    if 'train_encodings.pt' in files and 'train_labels.csv' in files:\n",
    "        print('read train dataset\\n')\n",
    "        train_encodings = torch.load('./class_inputData/train_encodings.pt')\n",
    "        train_labels = pd.read_csv('./class_inputData/train_labels.csv',index_col=0)\n",
    "        train_labels = train_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate train dataset\\n')\n",
    "        train_dataset,train_labels = get_input_from_sampling(train_size,train_flag=True)\n",
    "        train_encodings = tokenizer(train_dataset,padding=True,truncation=False,return_tensors=\"pt\")\n",
    "        torch.save(train_encodings, './class_inputData/train_encodings.pt')\n",
    "        pd.DataFrame(train_labels).to_csv('./class_inputData/train_labels.csv')\n",
    "        print('train dataset saved')\n",
    "        \n",
    "    print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "    \n",
    "    print('test dataset')\n",
    "    if 'test_encodings.pt' in files and 'test_labels.csv' in files:\n",
    "        print('read test dataset\\n')\n",
    "        test_encodings = torch.load('./class_inputData/test_encodings.pt')\n",
    "        test_labels = pd.read_csv('./class_inputData/test_labels.csv',index_col=0)\n",
    "        test_labels = test_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate test dataset\\n')\n",
    "        test_dataset,test_labels = get_input_from_sampling(test_size,train_flag=False)\n",
    "        test_encodings = tokenizer(test_dataset,padding=True,truncation=False,return_tensors=\"pt\")\n",
    "        torch.save(test_encodings, './class_inputData/test_encodings.pt')\n",
    "        pd.DataFrame(test_labels).to_csv('./class_inputData/test_labels.csv')\n",
    "        print('test dataset saved\\n')\n",
    "\n",
    "    # encode the labels\n",
    "    train_labels_encoded = list(map(lambda x:ClassEncoder(x),train_labels))\n",
    "    print(len(train_labels_encoded))\n",
    "    test_labels_encoded = list(map(lambda x:ClassEncoder(x),test_labels))\n",
    "    print(len(test_labels_encoded))\n",
    "    \n",
    "    return train_encodings,test_encodings,train_labels_encoded,test_labels_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassEncoder(Class):\n",
    "    global class_dict\n",
    "    return class_dict[Class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassDecoder(Class):\n",
    "    global class_dict\n",
    "    for key,val in class_dict.items():\n",
    "        if val == Class:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    global class_dict\n",
    "    print('compute_metrics:',len(labels))\n",
    "\n",
    "    res = pd.DataFrame({\"preds\":preds,\"labels\":labels})\n",
    "    res.to_csv('test_set_result.csv')\n",
    "    \n",
    "    class_preds = [ClassDecoder(item) for item in preds]\n",
    "    class_labels = [ClassDecoder(item) for item in labels]\n",
    "    class_report = classification_report(class_labels, class_preds,output_dict=True)\n",
    "    class_report2 = classification_report(class_labels, class_preds)\n",
    "    class_result = {'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall}\n",
    "    \n",
    "    with open('class_classification_report.txt','w') as f: \n",
    "        js = json.dumps(class_report) \n",
    "        f.write(js)\n",
    "    with open('class_classification_report_string_format.txt','w') as f: \n",
    "        f.write(class_report2)\n",
    "    with open('class_running_output.txt','w') as f:\n",
    "        f.write('class_result:'+str(class_result)+'\\n')\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    set_seed(1024)\n",
    "    model_checkpoint = \"albert-base-v2\"\n",
    "    # model_checkpoint = r'C:\\Users\\coldkiller\\Desktop\\supplyframe\\checkpoint-3500'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    #logging.basicConfig(level=logging.DEBUG,format='%(asctime)s %(message)s')\n",
    "    \n",
    "    print('torch.cuda.is_available()',gpu_available)\n",
    "    \n",
    "    train_encodings,test_encodings,train_labels_encoded,test_labels_encoded = input_process(config_classinfo,\n",
    "                                                                                    model_checkpoint,tokenizer,7,1,class_dict)\n",
    "\n",
    "    print('input_process_finished\\n')\n",
    "    train_dataset = processDataset(train_encodings, train_labels_encoded)\n",
    "    test_dataset = processDataset(test_encodings, test_labels_encoded)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=35)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=50,\n",
    "        save_steps=500000,\n",
    "        no_cuda= not gpu_available,\n",
    "        seed=1024,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=5,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print('Training begins\\n')\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    end = time.time()\n",
    "    print(f\"training time: {end - start}\")\n",
    "\n",
    "    train_result = trainer.evaluate(train_dataset)\n",
    "    print(train_result)\n",
    "    test_result = trainer.predict(test_dataset).metrics\n",
    "    print(test_result)\n",
    "\n",
    "    trainer.save_model()\n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write(f\"training time: {end - start}\"+'\\n')\n",
    "        f.write('train_dataset'+str(train_result)+'\\n')        \n",
    "        f.write('test_dataset'+str(test_result)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_class_mapping():\n",
    "    #加上unknown是36个class\n",
    "    class_dict = {}\n",
    "    i=1\n",
    "    for part_class in config_classinfo.keys():\n",
    "        class_dict[part_class] = i\n",
    "        i+=1\n",
    "\n",
    "    class_dict['unknown_class'] = 0\n",
    "\n",
    "    with open('class_dict.txt', 'w') as f:\n",
    "        dic = json.dumps(class_dict)  \n",
    "        f.write(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     training_args = TrainingArguments(\n",
    "#         output_dir='./results',\n",
    "#         learning_rate=1e-5,\n",
    "#         weight_decay=0.01,\n",
    "#         adam_beta1=0.9,\n",
    "#         adam_beta2=0.999,\n",
    "#         adam_epsilon=1e-8,\n",
    "#         num_train_epochs=1,\n",
    "#         logging_steps=5,\n",
    "#         evaluation_strategy='steps',\n",
    "#         save_steps=500000,\n",
    "#         no_cuda=False,\n",
    "#         seed=1024,\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=64,\n",
    "#         warmup_steps=500,\n",
    "#         logging_dir='./logs',\n",
    "#         load_best_model_at_end=True,\n",
    "#         save_total_limit=5,\n",
    "#         disable_tqdm=True\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_transformer",
   "language": "python",
   "name": "sim_transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
