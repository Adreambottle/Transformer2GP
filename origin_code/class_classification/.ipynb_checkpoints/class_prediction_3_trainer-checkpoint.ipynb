{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sampling\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sklearn\n",
    "import time\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,classification_report\n",
    "import json\n",
    "import gc\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class_features = sampling.read_data(\"config/config-class-features.json\")\n",
    "config_class_name = sampling.read_data(\"config/config-class-name.json\")\n",
    "config_classinfo = sampling.read_data(\"config/config-classinfo.json\")\n",
    "config_numeric_fields = sampling.read_data(\"config/config-numeric-fields.json\")\n",
    "config_dynamic_units = sampling.read_data(\"config/config-dynamic-units.json\")\n",
    "pair_params = sampling.read_data(\"config/pair-params\")\n",
    "class_dict = {'Resistors':1,'Capacitors':2,'others':0}\n",
    "with open(r'testDesc(cap&res).json', 'r', errors='ignore', encoding='utf-8') as f:\n",
    "    js = f.read()\n",
    "    raw_input = json.loads(js, strict=False, encoding=\"GB2312\")['descriptions']\n",
    "labels=[1]*404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'samples.json', 'r', errors='ignore', encoding='utf-8') as f:\n",
    "#     js = f.read()\n",
    "#     result = json.loads(js, strict=False, encoding=\"GB2312\")\n",
    "#3737470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进入数据生成循环\n",
      "train_flag True\n",
      "problematic set()\n",
      "query数量： 37\n",
      "label数量： 37\n",
      "('COMMERCIAL#6.5 5.0uV#36.0-1/2 BIDIRECTIONAL;0.011', 'Logic') \n",
      "\n",
      "('ALUMINUM ALLOY;CRIMP -55.0/MALE,14:COMPATIBLE CONTACT: 6862-201-20278:200.0_49', 'Connectors') \n",
      "\n",
      "('Automotive Circuits 12V;NO,13.75/8,8/6°C/3.0', 'Signal Circuits') \n",
      "\n",
      "('NOT SPECIFIED_6.985mm,BIPOLAR,Telecommunication Circuits_通讯电路;4.15uV_51840.0#STS-1/OC-1 SONET;SDH', 'Telecommunication Circuits') \n",
      "\n",
      "('M8C:4/6_ATA-2; SCSI 8.0,Microcontrollers and Processors-NOT SPECIFIED', 'Microcontrollers and Processors') \n",
      "\n",
      "('6/7,105.0/3 PPM/YEAR#13.79/4000.0ppm-SURFACE MOUNT:5/10MHz-7/3', 'Crystals/Resonators') \n",
      "\n",
      "('38535Q/M;38534H;883B/R-PDSO-G24#5/10 3/6kHz;-7.185V;10/10-50 OHM-SURFACE MOUNT/-13.0kV', 'Filters') \n",
      "\n",
      "(',x40-001LF24-27PW;NO#GOLD#PANEL YES:INCONEL 600-5#FEMALE-FEMALE', 'Connectors') \n",
      "\n",
      "('5.5kHz;TEMPERATURE STABILITY 3 PERCENT WITH AFC IS POSSIBLE -20.0mV_1500.0%,150.0mA,MECHANICAL TUNED CAVITY OSCILLATOR-3/6MHz/Oscillators:4700.0MHz', 'Oscillators') \n",
      "\n",
      "('-32.5:BULKHEAD-BULKHEAD,9,COPPER ALLOY:1:AUDIO/RCA CONNECTOR:97.5_UL', 'Connectors') \n",
      "\n",
      "('0.35kHz 130.0,1.5-3/5mm 250.0kHz:7/8 703.5', 'Transformers') \n",
      "\n",
      "('SSOP#1.0:1.1/5.7mm#Clock Drivers/2.9uV-2.0-22.5', 'Logic') \n",
      "\n",
      "(\"30:BINARY, 2'S COMPLEMENT BINARY/165.0MHz:1/1;11.25mV-THROUGH-HOLE_77.5µs\", 'Converters') \n",
      "\n",
      "('10/8°C_0.57Ah,52.5°C,IEC-BR2032, NEDA-5004LB/LITHIUM-ION/3.0uV;SECONDARY;CR201x;FH-LF/3.0V', 'Batteries') \n",
      "\n",
      "('75.0/2.0/2.205/1_3.0-Audio Control ICs 5.005%/RECTANGULAR', 'Consumer Circuits') \n",
      "\n",
      "('X7R-0805,-55.0°C;0.001µF/125.0°C-1.5,Capacitors', 'Capacitors') \n",
      "\n",
      "('5V IT CAN ALSO OPERATE WITH 6.6V TO 15V SINGLE SUPPLY#2/9mm_SIP/1.0#Other_10.0', 'Consumer Circuits') \n",
      "\n",
      "('VOLTAGE-MODE:-6.0,Signal YES_HYBRID:Analog Waveform Generation Functions UNSPECIFIED#+-6', 'Signal Circuits') \n",
      "\n",
      "('WMZ/60.0uV#2.1kA_2;Silver (Ag)#INSTANTANEOUS/245.0V', 'Circuit Protection') \n",
      "\n",
      "('115.0V#REEL_230/7/7;UL; CSA,1.65;84.15mm/7/1mm', 'Transformers') \n",
      "\n",
      "('2/1,NO LEAD#13-BIT/1.0;0.575mm 4/3', 'Telecommunication Circuits') \n",
      "\n",
      "('2.075ns:12.25-BIDIRECTIONAL-Logic;56 10/3_FCT', 'Logic') \n",
      "\n",
      "('PLASTIC/EPOXY;3:Consumer Circuits:2.4uV#DIE', 'Consumer Circuits') \n",
      "\n",
      "('NOT APPLICABLE:MO-078AA/Bridge Rectifier Diodes;SILICON/4/2,5.2/3.75kA', 'Diodes') \n",
      "\n",
      "('2/LIQUID CRYSTAL POLYMER (LCP)#BOARD STACKING CONNECTOR,2;MULTIPLE MATING PARTS AVAILABLE/BOARD/.139', 'Connectors') \n",
      "\n",
      "('4.15V:5.0_CMOS_80.0°C/3.3/5V_0.0°C', 'Telecommunication Circuits') \n",
      "\n",
      "('252.0kV/NOT SPECIFIED,30.0:ROUND,NO 4/4mA', 'Trigger Devices') \n",
      "\n",
      "('10.0/CERAMIC:2.9 9.0-2/2:Interfaces', 'Drivers And Interfaces') \n",
      "\n",
      "('5.0V 5962-8xx08,02QX#7/3/BIT-SLICE PROCESSOR, DIRECT MEMORY ADDRESS GENERATOR 16.0,CHIP CARRIER;28.125', 'Microcontrollers and Processors') \n",
      "\n",
      "('0.3µF/SMT:7/1mm-CERAMIC CAPACITOR/C0G/CERAMIC', 'Capacitors') \n",
      "\n",
      "('1.5/7.5#-55.0°C-4/5°C,SMA FEMALE;1250.0MHz,14900.0', 'Filters') \n",
      "\n",
      "('MIL-C-38999/650.0;CONNECTOR ACCESSORY,连接辅助;15.0#BACKSHELL', 'Connector Support') \n",
      "\n",
      "('330.0/Radial/1361.0µF:17.5_2626,3/8mΩ', 'Capacitors') \n",
      "\n",
      "('2.0%-70.0°C/0.065mW,0.232nW-4450.0R', 'Resistors') \n",
      "\n",
      "('4.0;29.5mA-放大电路,10.0µA_1000.0:1.905mm', 'Amplifier Circuits') \n",
      "\n",
      "('77.45MHz,Signal Circuits;NO;QCCN,-10.0V,WIRE_+-5 e0', 'Signal Circuits') \n",
      "\n",
      "('50 OHM#SMA FEMALE:Filters_Ceramic Filters#1.5;5.0/5.25mm CERAMIC BPF', 'Filters') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#19个resistors的文件,12个capacitors的文件，总共374个文件\n",
    "def convert2str(label_list,item):\n",
    "    tmp_string = ''\n",
    "\n",
    "    shuffled_item = list(item.items())\n",
    "    random.shuffle(shuffled_item)\n",
    "    \n",
    "    text, text_sep= \"\", \"\"\n",
    "    deliminter_list = ['#', ',', '/', ';', ':', '-', '_',' ']\n",
    "    deliminter = random.sample(deliminter_list, 1)[0]\n",
    "    \n",
    "    for (key, val) in shuffled_item:\n",
    "        if key == 'category':\n",
    "            continue\n",
    "        elif key == 'class':\n",
    "            label_list.append(val)\n",
    "        else:\n",
    "            if random.uniform(0, 1) > 0.1: # 10% chance to use another deliminter\n",
    "                tmp_string += str(val) + random.sample(deliminter_list, 1)[0]\n",
    "            else:\n",
    "                tmp_string += str(val) + deliminter\n",
    "                \n",
    "    output = re.sub(\"(\" + \"|\".join(deliminter_list) + \")$\", \"\", tmp_string)\n",
    "    return output\n",
    "\n",
    "def get_input_from_sampling(input_size: int,train_flag:True):\n",
    "    \"\"\"\n",
    "    input_size: the number of items to sample\n",
    "    returns the input for simpletransformer\n",
    "    \"\"\"\n",
    "    # 读取所有catogory文件\n",
    "    path = os.getcwd()\n",
    "    files= os.listdir('./formatData') #得到文件夹下的所有文件名称\n",
    "    rs_list = []\n",
    "    \n",
    "    # input_size为1表示从每一个category抽取一个样本\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    problematic = set()\n",
    "    \n",
    "    print('进入数据生成循环')\n",
    "    for file in files: #遍历文件夹\n",
    "        file_path = os.path.join(path, 'formatData/'+file)\n",
    "        if os.path.isfile(file_path): #判断是否是文件夹，不是文件夹才打开\n",
    "            rs = sampling.sampling(file,0.6)\n",
    "            if 'Resistors' in file:\n",
    "                for i in range(18*input_size):\n",
    "                    try:\n",
    "                        item = [item for item in rs.random_sampling()][0]\n",
    "                        text_list.append(convert2str(label_list,item)) \n",
    "                    except Exception as e:\n",
    "                        problematic.add(file)\n",
    "                        continue\n",
    "\n",
    "            elif 'Capacitors' in file:\n",
    "                for i in range(29*input_size):\n",
    "                    try:\n",
    "                        item = [item for item in rs.random_sampling()][0]\n",
    "                        text_list.append(convert2str(label_list,item))\n",
    "                    except Exception as e:\n",
    "                        problematic.add(file)\n",
    "                        continue\n",
    "                        \n",
    "            else:\n",
    "                for i in range(input_size):\n",
    "                    try:\n",
    "                        item = [item for item in rs.random_sampling()][0]\n",
    "                        text_list.append(convert2str(label_list,item))\n",
    "                    except Exception as e:\n",
    "                        problematic.add(file)\n",
    "                        continue\n",
    "    \n",
    "    #to shuffle input\n",
    "    res=[]\n",
    "    for i in range(len(label_list)):\n",
    "        res.append((text_list[i],label_list[i]))\n",
    "    random.shuffle(res)\n",
    "    for i in range(len(label_list)):\n",
    "        text_list[i]=res[i][0]\n",
    "        label_list[i]=res[i][1]\n",
    "        \n",
    "    print('train_flag',train_flag)\n",
    "    print('problematic',problematic)\n",
    "    print('query数量：',len(text_list))\n",
    "    print('label数量：',len(label_list))\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write('train_flag:'+str(train_flag)+'\\n')\n",
    "        f.write('query数量:'+str(len(text_list))+'\\n')\n",
    "        f.write('label数量:'+str(len(label_list))+'\\n')\n",
    "        f.write('problematic:'+str(problematic)+'\\n')\n",
    "    return text_list,label_list\n",
    "\n",
    "# a,b=get_input_from_sampling(1,True)\n",
    "# for i in range(len(b)):\n",
    "#     print((a[i],b[i]),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_process(config_classinfo,model_checkpoint,train_size,class_dict,tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('script running\\n')\n",
    "    files= os.listdir('./class_inputData')\n",
    "\n",
    "    with open('running_output.txt','w') as f:\n",
    "        f.write(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())+'\\n')\n",
    "    \n",
    "    #准备训练数据\n",
    "    print('train dataset\\n')\n",
    "    if 'train_encodings.pt' in files and 'train_labels.csv' in files:\n",
    "        print('read train dataset\\n')\n",
    "        train_encodings = torch.load('./class_inputData/train_encodings.pt')\n",
    "        train_labels = pd.read_csv('./class_inputData/train_labels.csv',index_col=0)\n",
    "        train_labels = train_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate train dataset\\n')\n",
    "        train_dataset,train_labels = get_input_from_sampling(train_size,train_flag=True)\n",
    "        \n",
    "        print('begin train tokenizer')\n",
    "        train_encodings = tokenizer(train_dataset,padding=True,truncation=False)\n",
    "        print('finish train tokenizer')\n",
    "        \n",
    "        pd.DataFrame(train_labels).to_csv('./class_inputData/train_labels.csv')\n",
    "        torch.save(train_encodings, './class_inputData/train_encodings.pt')\n",
    "        print('train dataset saved')\n",
    "        \n",
    "    # encode the labels\n",
    "    train_labels_encoded = list(map(lambda x:ClassEncoder(x),train_labels))\n",
    "    print(len(train_labels_encoded))\n",
    "    return train_encodings,train_labels_encoded\n",
    "\n",
    "def test_input_process(config_classinfo,model_checkpoint,test_size,class_dict,tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('test dataset')\n",
    "    files= os.listdir('./class_inputData')\n",
    "    \n",
    "    if 'test_encodings.pt' in files and 'test_labels.csv' in files:\n",
    "        print('read test dataset\\n')\n",
    "        test_encodings = torch.load('./class_inputData/test_encodings.pt')\n",
    "        test_labels = pd.read_csv('./class_inputData/test_labels.csv',index_col=0)\n",
    "        test_labels = test_labels['0'].to_list()\n",
    "    else:\n",
    "        print('generate test dataset\\n')\n",
    "        test_dataset,test_labels = get_input_from_sampling(test_size,train_flag=False)\n",
    "        \n",
    "        print('begin test tokenizer')\n",
    "        test_encodings = tokenizer(test_dataset,padding=True,truncation=False)\n",
    "\n",
    "        print('finish test tokenizer')\n",
    "        torch.save(test_encodings, './class_inputData/test_encodings.pt')\n",
    "        pd.DataFrame(test_labels).to_csv('./class_inputData/test_labels.csv')\n",
    "        print('test dataset saved\\n')\n",
    "\n",
    "    # encode the labels\n",
    "    test_labels_encoded = list(map(lambda x:ClassEncoder(x),test_labels))\n",
    "    print(len(test_labels_encoded))\n",
    "    \n",
    "    return test_encodings,test_labels_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class processDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class processDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data, labels,tokenizer):\n",
    "#         self.data=data\n",
    "#         self.tokenizer=tokenizer\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         encodings=self.tokenizer(self.data, padding=True, truncation=False)\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassEncoder(Class):\n",
    "    if Class == 'Resistors':\n",
    "        return 1\n",
    "    elif Class == 'Capacitors':\n",
    "        return 2\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassDecoder(Class):\n",
    "    if Class == 1:\n",
    "        return 'Resistors'\n",
    "    elif Class == 2:\n",
    "        return 'Capacitors'\n",
    "    else:\n",
    "        return 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    global class_dict\n",
    "    print('compute_metrics:',len(labels))\n",
    "\n",
    "    res = pd.DataFrame({\"preds\":preds,\"labels\":labels})\n",
    "    res[:1000].to_csv('test_set_result.csv')\n",
    "    \n",
    "    class_preds = [ClassDecoder(item) for item in preds]\n",
    "    class_labels = [ClassDecoder(item) for item in labels]\n",
    "    class_report = classification_report(class_labels, class_preds,output_dict=True)\n",
    "    class_report2 = classification_report(class_labels, class_preds)\n",
    "    class_result = {'accuracy': acc,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall}\n",
    "    \n",
    "    with open('class_classification_report.json','w',errors='ignore') as f: \n",
    "        json.dump(class_report,f,ensure_ascii=False, indent = 4) \n",
    "#     with open('class_classification_report_string_format.txt','w') as f: \n",
    "#         f.write(class_report2)\n",
    "    with open('class_running_output.txt','w') as f:\n",
    "        f.write('class_result:'+str(class_result)+'\\n')\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5064bf6f4c1c48b08a1a1c3dcebbf85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=285.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1592ae6d0348edbb340752b1f58e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.cuda.is_available() False\n",
      "script running\n",
      "\n",
      "train dataset\n",
      "\n",
      "generate train dataset\n",
      "\n",
      "进入数据生成循环\n",
      "train_flag True\n",
      "problematic set()\n",
      "query数量： 414\n",
      "label数量： 414\n",
      "sys.getsizeof(train_dataset) 3760\n",
      "train dataset saved\n",
      "414\n",
      "test dataset\n",
      "generate test dataset\n",
      "\n",
      "进入数据生成循环\n",
      "train_flag False\n",
      "problematic set()\n",
      "query数量： 414\n",
      "label数量： 414\n",
      "test dataset saved\n",
      "\n",
      "414\n",
      "input_process_finished\n",
      "\n",
      "sys.getsizeof(train_data) 3760\n",
      "sys.getsizeof(test_data) 3760\n",
      "sys.getsizeof(train_dataset) 56\n",
      "sys.getsizeof(test_dataset) 56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aee924c09140db90cefaac5eabf9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=17756393.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training begins\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2bf9728d21a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training begins\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"training time: {end - start}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model_path, trial)\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-e4f1be547f3c>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mencodings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2212\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2213\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2214\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2215\u001b[0m             )\n\u001b[0;32m   2216\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2397\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2399\u001b[1;33m             \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2400\u001b[0m         )\n\u001b[0;32m   2401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    547\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                 \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[1;34m(tok_list, text)\u001b[0m\n\u001b[0;32m    344\u001b[0m                     (\n\u001b[0;32m    345\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m                     )\n\u001b[0;32m    348\u001b[0m                 )\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    344\u001b[0m                     (\n\u001b[0;32m    345\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m                     )\n\u001b[0;32m    348\u001b[0m                 )\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# If the token is part of the never_split set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[0msplit_tokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mchar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[0mstart_new_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\sim_transformer\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_is_punctuation\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m33\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m47\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m58\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m91\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m96\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m123\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m126\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"P\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "\n",
    "    set_seed(1024)\n",
    "    #model_checkpoint = \"albert-base-v2\"\n",
    "    model_checkpoint = \"prajjwal1/bert-tiny\"\n",
    "    # model_checkpoint = r'C:\\Users\\coldkiller\\Desktop\\supplyframe\\checkpoint-3500'\n",
    "    \n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    #logging.basicConfig(level=logging.DEBUG,format='%(asctime)s %(message)s')\n",
    "    \n",
    "    print('torch.cuda.is_available()',gpu_available)\n",
    "    \n",
    "    train_data,train_labels_encoded = train_input_process(config_classinfo,model_checkpoint,100,class_dict,tokenizer)\n",
    "    test_data,test_labels_encoded = test_input_process(config_classinfo,model_checkpoint,10,class_dict,tokenizer)\n",
    "\n",
    "    print('input_process_finished\\n')\n",
    "    print('sys.getsizeof(train_data)',sys.getsizeof(train_data))\n",
    "    print('sys.getsizeof(test_data)',sys.getsizeof(test_data))\n",
    "    \n",
    "    train_dataset = processDataset(train_data, train_labels_encoded)\n",
    "    test_dataset = processDataset(test_data, test_labels_encoded)\n",
    "    real_dataset = processDataset(raw_input,labels)\n",
    "    print('sys.getsizeof(train_dataset)',sys.getsizeof(train_dataset))\n",
    "    print('sys.getsizeof(test_dataset)',sys.getsizeof(test_dataset))\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        dataloader_num_workers=7,\n",
    "        do_train = True,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "        num_train_epochs=20,\n",
    "        logging_steps=1,\n",
    "        save_steps=300,\n",
    "        no_cuda= not gpu_available,\n",
    "        seed=1024,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=3,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=5,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print('Training begins\\n')\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    end = time.time()\n",
    "    print(f\"training time: {end - start}\")\n",
    "\n",
    "    trainer.save_model()\n",
    "    \n",
    "    train_result = trainer.predict(train_dataset).metrics\n",
    "    print(train_result)\n",
    "    test_result = trainer.predict(test_dataset).metrics\n",
    "    print(test_result)\n",
    "    \n",
    "    real_result = trainer.predict(real_dataset).predictions.argmax(-1)\n",
    "    class_result = {}\n",
    "    for i in range(len(input)):\n",
    "        class_result[input[i]]=str(preds[i])\n",
    "    \n",
    "    with open('running_output.txt','a') as f:\n",
    "        f.write(f\"training time: {end - start}\"+'\\n')\n",
    "        f.write('train_dataset'+str(train_result)+'\\n')        \n",
    "        f.write('test_dataset'+str(test_result)+'\\n')\n",
    "        \n",
    "    with open('real_description_output1.json','w',errors='ignore') as f: \n",
    "        json.dump(class_result,f,ensure_ascii=False, indent = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_class_mapping():\n",
    "    #加上unknown是36个class\n",
    "    class_dict = {}\n",
    "    i=1\n",
    "    for part_class in config_classinfo.keys():\n",
    "        class_dict[part_class] = i\n",
    "        i+=1\n",
    "\n",
    "    class_dict['unknown_class'] = 0\n",
    "\n",
    "    with open('class_dict.txt', 'w') as f:\n",
    "        dic = json.dumps(class_dict)  \n",
    "        f.write(dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim_transformer",
   "language": "python",
   "name": "sim_transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
